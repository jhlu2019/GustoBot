# GustoBot çˆ¬è™«ä½¿ç”¨æŒ‡å—

## ğŸ“š ç›®å½•

- [æ¦‚è¿°](#æ¦‚è¿°)
- [å¿«é€Ÿå¼€å§‹](#å¿«é€Ÿå¼€å§‹)
- [çˆ¬è™«ç±»å‹](#çˆ¬è™«ç±»å‹)
- [ä»£ç†æ± é…ç½®](#ä»£ç†æ± é…ç½®)
- [å‘½ä»¤è¡Œå·¥å…·](#å‘½ä»¤è¡Œå·¥å…·)
- [è‡ªå®šä¹‰çˆ¬è™«å¼€å‘](#è‡ªå®šä¹‰çˆ¬è™«å¼€å‘)
- [æ•°æ®éªŒè¯ä¸æ¸…æ´—](#æ•°æ®éªŒè¯ä¸æ¸…æ´—)
- [æœ€ä½³å®è·µ](#æœ€ä½³å®è·µ)
- [å¸¸è§é—®é¢˜](#å¸¸è§é—®é¢˜)

---

## æ¦‚è¿°

GustoBotçˆ¬è™«æ¨¡å—æ˜¯ä¸€ä¸ªä¼ä¸šçº§çš„ç½‘é¡µæ•°æ®é‡‡é›†æ¡†æ¶ï¼Œä¸“ä¸ºèœè°±æ•°æ®é‡‡é›†è®¾è®¡ï¼Œä½†å¯æ‰©å±•ç”¨äºå…¶ä»–ç±»å‹ç½‘ç«™ã€‚

### æ ¸å¿ƒç‰¹æ€§

âœ… **åŒå¼•æ“æ¶æ„**
- **HTTPçˆ¬è™«** (BaseCrawler): åŸºäºhttpxï¼Œé€‚ç”¨äºé™æ€é¡µé¢ï¼Œé€Ÿåº¦å¿«
- **æµè§ˆå™¨çˆ¬è™«** (BrowserCrawler): åŸºäºPlaywrightï¼Œé€‚ç”¨äºåŠ¨æ€é¡µé¢ï¼ŒåŠŸèƒ½å¼ºå¤§

âœ… **å®Œå–„çš„åçˆ¬æœºåˆ¶**
- IPä»£ç†æ± ï¼ˆè‡ªåŠ¨è½®æ¢ã€å¥åº·æ£€æŸ¥ï¼‰
- éšæœºUser-Agentæ± 
- è¯·æ±‚å»¶è¿Ÿï¼ˆå¯é…ç½®èŒƒå›´ï¼‰
- å¤±è´¥é‡è¯•ï¼ˆæŒ‡æ•°é€€é¿ï¼‰
- Robots.txtéµå®ˆ

âœ… **æ•°æ®è´¨é‡ä¿éšœ**
- Pydanticæ¨¡å‹éªŒè¯
- è‡ªåŠ¨æ•°æ®æ¸…æ´—
- å»é‡å¤„ç†
- æ—¶é—´æ ¼å¼è§„èŒƒåŒ–

âœ… **å¤šæ•°æ®æºæ”¯æŒ**
- Wikipedia API
- Schema.orgæ ‡å‡†ç½‘ç«™ï¼ˆJSON-LDã€Microdataï¼‰
- é€šç”¨HTMLé¡µé¢ï¼ˆå¯å‘å¼è§£æï¼‰

---

## å¿«é€Ÿå¼€å§‹

### å®‰è£…ä¾èµ–

çˆ¬è™«æ¨¡å—éœ€è¦é¢å¤–çš„ä¾èµ–ï¼š

```bash
# åŸºç¡€HTTPçˆ¬è™«
pip install httpx beautifulsoup4 lxml fake-useragent pydantic

# æµè§ˆå™¨çˆ¬è™«ï¼ˆéœ€è¦å®‰è£…Playwrightï¼‰
pip install playwright
playwright install chromium  # å®‰è£…Chromiumæµè§ˆå™¨
```

### ç¬¬ä¸€ä¸ªçˆ¬è™«ç¤ºä¾‹

**ç¤ºä¾‹1: ä½¿ç”¨å‘½ä»¤è¡Œå·¥å…·çˆ¬å–Wikipedia**

```bash
python -m gustobot.crawler.cli wikipedia --query "çº¢çƒ§è‚‰" --limit 5
```

**ç¤ºä¾‹2: ä½¿ç”¨Pythonä»£ç **

```python
import asyncio
from gustobot.crawler import WikipediaCrawler

async def main():
    # åˆ›å»ºçˆ¬è™«å®ä¾‹
    crawler = WikipediaCrawler(language="zh")

    # æ‰§è¡Œçˆ¬å–
    recipes = await crawler.run(
        search_queries=["å·èœ", "ç²¤èœ"],
        limit_per_query=5
    )

    print(f"æˆåŠŸçˆ¬å– {len(recipes)} ä¸ªèœè°±")
    for recipe in recipes:
        print(f"- {recipe['name']}")

if __name__ == "__main__":
    asyncio.run(main())
```

---

## çˆ¬è™«ç±»å‹

### 1. HTTPçˆ¬è™« (BaseCrawler)

**é€‚ç”¨åœºæ™¯**: é™æ€HTMLé¡µé¢ï¼Œå†…å®¹ç›´æ¥åŒ…å«åœ¨HTMLæºç ä¸­

**ä¼˜ç‚¹**: é€Ÿåº¦å¿«ï¼Œèµ„æºæ¶ˆè€—ä½

**ç¼ºç‚¹**: æ— æ³•å¤„ç†JavaScriptåŠ¨æ€å†…å®¹

**ç¤ºä¾‹**:

```python
from gustobot.crawler import BaseCrawler
import httpx

class SimpleRecipeCrawler(BaseCrawler):
    def __init__(self):
        super().__init__(
            name="SimpleRecipeCrawler",
            request_delay=(1, 3),  # 1-3ç§’éšæœºå»¶è¿Ÿ
            max_retries=3
        )

    async def parse(self, response: httpx.Response):
        from bs4 import BeautifulSoup
        soup = BeautifulSoup(response.text, 'html.parser')

        # æå–æ•°æ®
        recipe = {
            "name": soup.find('h1', class_='title').text.strip(),
            "ingredients": [li.text for li in soup.find_all('li', class_='ingredient')],
            "steps": [p.text for p in soup.find_all('p', class_='step')]
        }
        return recipe

    async def run(self, urls):
        self.start_stats()
        recipes = []

        for url in urls:
            response = await self.fetch(url)
            if response:
                recipe = await self.parse(response)
                recipes.append(recipe)
                self.stats["items_scraped"] += 1

        self.end_stats()
        return recipes
```

### 2. æµè§ˆå™¨çˆ¬è™« (BrowserCrawler)

**é€‚ç”¨åœºæ™¯**: éœ€è¦JavaScriptæ¸²æŸ“çš„åŠ¨æ€é¡µé¢

**ä¼˜ç‚¹**: åŠŸèƒ½å¼ºå¤§ï¼Œå¯æ¨¡æ‹Ÿç”¨æˆ·è¡Œä¸º

**ç¼ºç‚¹**: é€Ÿåº¦æ…¢ï¼Œèµ„æºæ¶ˆè€—é«˜

**æ ¸å¿ƒåŠŸèƒ½**:
- âœ… JavaScriptæ¸²æŸ“
- âœ… é¡µé¢æ»šåŠ¨ï¼ˆè§¦å‘æ‡’åŠ è½½ï¼‰
- âœ… å…ƒç´ ç‚¹å‡»ï¼ˆå±•å¼€æŒ‰é’®ç­‰ï¼‰
- âœ… ç­‰å¾…å…ƒç´ åŠ è½½
- âœ… æ‰§è¡Œè‡ªå®šä¹‰JSä»£ç 
- âœ… é¡µé¢æˆªå›¾

**ç¤ºä¾‹**:

```python
from gustobot.crawler.browser_crawler import BrowserCrawler
from lxml import etree

class DynamicRecipeCrawler(BrowserCrawler):
    def __init__(self, **kwargs):
        super().__init__(
            name="DynamicRecipeCrawler",
            headless=True,           # æ— å¤´æ¨¡å¼
            request_delay=(2, 4),    # 2-4ç§’å»¶è¿Ÿ
            **kwargs
        )

    async def parse(self, html_content: str, url: str):
        tree = etree.HTML(html_content)

        recipe = {
            "name": tree.xpath('//h1[@class="title"]/text()')[0].strip(),
            "ingredients": tree.xpath('//div[@class="ingredients"]//li/text()'),
            "steps": tree.xpath('//div[@class="steps"]//p/text()'),
            "url": url
        }
        return [recipe]

    async def run(self, urls):
        self.start_stats()
        recipes = []

        for url in urls:
            # åŠ è½½é¡µé¢ï¼Œæ»šåŠ¨3æ¬¡ï¼Œç‚¹å‡»"å±•å¼€"æŒ‰é’®
            html = await self.fetch_page(
                url,
                wait_selector='div.recipe-content',  # ç­‰å¾…å†…å®¹åŠ è½½
                scroll_count=3,                       # æ»šåŠ¨3æ¬¡
                click_selectors=[                     # ç‚¹å‡»å±•å¼€æŒ‰é’®
                    '//button[contains(text(), "å±•å¼€")]',
                    '//button[contains(text(), "æŸ¥çœ‹æ›´å¤š")]'
                ]
            )

            if html:
                parsed = await self.parse(html, url)
                recipes.extend(parsed)
                self.stats["items_scraped"] += len(parsed)

        self.end_stats()
        return recipes

# ä½¿ç”¨ç¤ºä¾‹
async def main():
    crawler = DynamicRecipeCrawler(headless=True)

    # ä½¿ç”¨async withè‡ªåŠ¨ç®¡ç†æµè§ˆå™¨ç”Ÿå‘½å‘¨æœŸ
    async with crawler:
        recipes = await crawler.run([
            "https://example.com/recipe1",
            "https://example.com/recipe2"
        ])

    print(f"çˆ¬å–äº† {len(recipes)} ä¸ªèœè°±")
```

### 3. æ··åˆçˆ¬è™« (HybridCrawler)

ç»“åˆHTTPçˆ¬è™«å’Œæµè§ˆå™¨çˆ¬è™«çš„ä¼˜åŠ¿ï¼š
- åˆ—è¡¨é¡µç”¨HTTPï¼ˆå¿«é€Ÿï¼‰
- è¯¦æƒ…é¡µç”¨æµè§ˆå™¨ï¼ˆåŠŸèƒ½å¼ºå¤§ï¼‰

```python
from gustobot.crawler.browser_crawler import HybridCrawler

class HybridRecipeCrawler(HybridCrawler):
    async def run(self, list_url):
        # ç¬¬ä¸€é˜¶æ®µ: ç”¨HTTPå¿«é€Ÿè·å–åˆ—è¡¨é¡µ
        list_html = await self.fetch_static(list_url)
        recipe_urls = self.extract_urls(list_html)

        # ç¬¬äºŒé˜¶æ®µ: ç”¨æµè§ˆå™¨æ¸²æŸ“è¯¦æƒ…é¡µ
        recipes = []
        for url in recipe_urls:
            html = await self.fetch_page(url, scroll_count=2)
            recipes.extend(await self.parse(html, url))

        return recipes
```

---

## ä»£ç†æ± é…ç½®

### ä¸ºä»€ä¹ˆéœ€è¦ä»£ç†æ± ï¼Ÿ

1. **é¿å…IPå°ç¦**: é¢‘ç¹è¯·æ±‚åŒä¸€ç½‘ç«™å¯èƒ½è¢«å°IP
2. **çªç ´è®¿é—®é™åˆ¶**: æŸäº›ç½‘ç«™é™åˆ¶å•IPè¯·æ±‚é¢‘ç‡
3. **åœ°åŸŸé™åˆ¶**: è®¿é—®æœ‰åœ°åŸŸé™åˆ¶çš„å†…å®¹

### ä»£ç†é…ç½®æ ¼å¼

åˆ›å»º`proxies.txt`æ–‡ä»¶ï¼Œæ”¯æŒå¤šç§æ ¼å¼ï¼š

```txt
# æ ¼å¼1: host:port
127.0.0.1:8080
192.168.1.100:3128

# æ ¼å¼2: host:port:username:password
proxy.example.com:8080:myuser:mypass

# æ ¼å¼3: protocol://host:port
http://127.0.0.1:8080
https://proxy.example.com:443

# æ ¼å¼4: protocol://username:password@host:port
http://user:pass@proxy.example.com:8080
socks5://user:pass@socks-proxy.com:1080

# æ³¨é‡Šè¡Œï¼ˆä»¥#å¼€å¤´ï¼‰ä¼šè¢«å¿½ç•¥
# ç©ºè¡Œä¹Ÿä¼šè¢«å¿½ç•¥
```

### ä½¿ç”¨ä»£ç†æ± 

**æ–¹æ³•1: ä»æ–‡ä»¶åŠ è½½**

```python
from gustobot.crawler.proxy_pool import ProxyPool
from gustobot.crawler.browser_crawler import BrowserCrawler

# åŠ è½½ä»£ç†æ± 
proxy_pool = ProxyPool.from_file("proxies.txt")

# åœ¨çˆ¬è™«ä¸­ä½¿ç”¨
crawler = BrowserCrawler(
    name="MyCrawler",
    proxy_pool=proxy_pool
)
```

**æ–¹æ³•2: æ‰‹åŠ¨æ·»åŠ **

```python
from gustobot.crawler.proxy_pool import ProxyPool

proxy_pool = ProxyPool(
    check_interval=300,   # å¥åº·æ£€æŸ¥é—´éš”(ç§’)
    max_fail_count=5,     # æœ€å¤§å¤±è´¥æ¬¡æ•°
    timeout=10.0          # ä»£ç†æµ‹è¯•è¶…æ—¶
)

# æ·»åŠ ä»£ç†
proxy_pool.add_proxy(host="127.0.0.1", port=8080)
proxy_pool.add_proxy(
    host="proxy.example.com",
    port=8080,
    username="user",
    password="pass"
)
```

### ä»£ç†æ± ç®¡ç†

```python
import asyncio
from gustobot.crawler.proxy_pool import ProxyPool

async def manage_proxies():
    proxy_pool = ProxyPool.from_file("proxies.txt")

    # æ‰‹åŠ¨å¥åº·æ£€æŸ¥
    await proxy_pool.health_check()

    # æŸ¥çœ‹ç»Ÿè®¡ä¿¡æ¯
    stats = proxy_pool.get_stats()
    print(f"æ€»ä»£ç†æ•°: {stats['total_proxies']}")
    print(f"æ´»è·ƒä»£ç†: {stats['active_proxies']}")
    print(f"å¹³å‡æˆåŠŸç‡: {stats['average_success_rate']:.2%}")
    print(f"å¹³å‡å“åº”æ—¶é—´: {stats['average_response_time']:.2f}s")

    # å¯åŠ¨è‡ªåŠ¨å¥åº·æ£€æŸ¥å¾ªç¯ï¼ˆæ¯5åˆ†é’Ÿï¼‰
    asyncio.create_task(proxy_pool.start_health_check_loop())
```

---

## å‘½ä»¤è¡Œå·¥å…·

### Wikipediaçˆ¬å–

```bash
# åŸºç¡€ç”¨æ³•
python -m gustobot.crawler.cli wikipedia --query "å·èœ" "ç²¤èœ"

# æŒ‡å®šè¯­è¨€å’Œæ•°é‡
python -m gustobot.crawler.cli wikipedia \
  --query "ä¸­å›½èœ" \
  --language zh \
  --limit 10

# ä½¿ç”¨ä»£ç†
python -m gustobot.crawler.cli wikipedia \
  --query "çƒ˜ç„™" \
  --proxy proxies.txt

# ä¿å­˜åˆ°æ–‡ä»¶
python -m gustobot.crawler.cli wikipedia \
  --query "å®¶å¸¸èœ" \
  --output recipes.json

# ç›´æ¥å¯¼å…¥åˆ°çŸ¥è¯†åº“
python -m gustobot.crawler.cli wikipedia \
  --query "ç”œå“" \
  --import-kb
```

### URLçˆ¬å–

```bash
# çˆ¬å–æŒ‡å®šURL
python -m gustobot.crawler.cli urls \
  --urls "https://example.com/recipe1" "https://example.com/recipe2"

# ä½¿ç”¨ä»£ç†å¹¶ä¿å­˜
python -m gustobot.crawler.cli urls \
  --urls "https://example.com/recipes" \
  --proxy proxies.txt \
  --output output.json

# ç›´æ¥å¯¼å…¥çŸ¥è¯†åº“
python -m gustobot.crawler.cli urls \
  --urls "https://example.com/recipe" \
  --import-kb
```

### ä»æ–‡ä»¶å¯¼å…¥

```bash
# å°†JSONæ–‡ä»¶å¯¼å…¥çŸ¥è¯†åº“
python -m gustobot.crawler.cli import \
  --file recipes.json \
  --batch-size 20
```

---

## è‡ªå®šä¹‰çˆ¬è™«å¼€å‘

### å¼€å‘æ­¥éª¤

1. **é€‰æ‹©åŸºç±»**: `BaseCrawler` (HTTP) æˆ– `BrowserCrawler` (æµè§ˆå™¨)
2. **å®ç°parseæ–¹æ³•**: è§£æHTMLæå–æ•°æ®
3. **å®ç°runæ–¹æ³•**: çˆ¬å–é€»è¾‘
4. **é…ç½®åçˆ¬å‚æ•°**: ä»£ç†ã€å»¶è¿Ÿã€é‡è¯•ç­‰

### å®Œæ•´ç¤ºä¾‹: ç¾é£Ÿå¤©ä¸‹çˆ¬è™«

```python
from gustobot.crawler.browser_crawler import BrowserCrawler
from gustobot.crawler.proxy_pool import ProxyPool
from lxml import etree
from typing import List, Dict
from loguru import logger

class MeishitianxiaCrawler(BrowserCrawler):
    """ç¾é£Ÿå¤©ä¸‹èœè°±çˆ¬è™«

    ç½‘ç«™ç‰¹ç‚¹:
    - åˆ—è¡¨é¡µ: åˆ†é¡µæ˜¾ç¤ºï¼Œæ¯é¡µ15ä¸ªèœè°±
    - è¯¦æƒ…é¡µ: éœ€è¦æ»šåŠ¨åŠ è½½å®Œæ•´æ­¥éª¤å›¾
    - åçˆ¬: æœ‰è®¿é—®é¢‘ç‡é™åˆ¶ï¼Œéœ€è¦ä»£ç†æ± 
    """

    def __init__(self, **kwargs):
        super().__init__(
            name="MeishitianxiaCrawler",
            headless=True,           # æ— å¤´æ¨¡å¼
            request_delay=(3, 6),    # 3-6ç§’éšæœºå»¶è¿Ÿï¼ˆé¿å…è¢«å°ï¼‰
            max_retries=3,           # æœ€å¤§é‡è¯•3æ¬¡
            timeout=60000,           # 60ç§’è¶…æ—¶
            **kwargs
        )

    async def parse_list_page(self, html_content: str) -> List[str]:
        """è§£æåˆ—è¡¨é¡µï¼Œæå–èœè°±é“¾æ¥"""
        tree = etree.HTML(html_content)

        # XPathæå–é“¾æ¥
        links = tree.xpath('//div[@class="recipe-item"]//a/@href')

        # è¡¥å…¨URL
        full_urls = [
            f"https://www.meishitianxia.com{link}"
            if link.startswith('/') else link
            for link in links
        ]

        logger.info(f"ä»åˆ—è¡¨é¡µæå–äº† {len(full_urls)} ä¸ªé“¾æ¥")
        return full_urls

    async def parse(self, html_content: str, url: str) -> List[Dict]:
        """è§£æè¯¦æƒ…é¡µï¼Œæå–èœè°±æ•°æ®"""
        tree = etree.HTML(html_content)

        try:
            recipe = {
                "name": tree.xpath('//h1[@class="recipe-title"]/text()')[0].strip(),
                "category": tree.xpath('//span[@class="category"]/text()')[0].strip(),
                "difficulty": tree.xpath('//span[@class="difficulty"]/text()')[0].strip(),
                "time": tree.xpath('//span[@class="time"]/text()')[0].strip(),

                # é£Ÿæåˆ—è¡¨
                "ingredients": [
                    ing.strip()
                    for ing in tree.xpath('//div[@class="ingredients"]//li/text()')
                    if ing.strip()
                ],

                # æ­¥éª¤ï¼ˆåŒ…å«å›¾ç‰‡ï¼‰
                "steps": [
                    {
                        "step": i + 1,
                        "description": step.strip(),
                        "image": tree.xpath(f'//div[@class="steps"]//div[@data-step="{i+1}"]//img/@src')
                    }
                    for i, step in enumerate(tree.xpath('//div[@class="steps"]//p/text()'))
                ],

                # å°è´´å£«
                "tips": tree.xpath('//div[@class="tips"]/text()')[0].strip()
                        if tree.xpath('//div[@class="tips"]/text()') else "",

                # ä¸»å›¾
                "image": tree.xpath('//img[@class="recipe-main-img"]/@src')[0],

                # å…ƒæ•°æ®
                "url": url,
                "source": "ç¾é£Ÿå¤©ä¸‹"
            }

            logger.info(f"æˆåŠŸè§£æèœè°±: {recipe['name']}")
            return [recipe]

        except Exception as e:
            logger.error(f"è§£æå¤±è´¥ {url}: {e}")
            return []

    async def run(
        self,
        category_urls: List[str],
        max_recipes: int = 100,
        save_to_db: bool = False
    ) -> List[Dict]:
        """
        è¿è¡Œçˆ¬è™«

        Args:
            category_urls: åˆ†ç±»é¡µURLåˆ—è¡¨
            max_recipes: æœ€å¤§çˆ¬å–æ•°é‡
            save_to_db: æ˜¯å¦ä¿å­˜åˆ°æ•°æ®åº“

        Returns:
            èœè°±åˆ—è¡¨
        """
        self.start_stats()
        all_recipes = []

        # ç¬¬ä¸€é˜¶æ®µ: è·å–æ‰€æœ‰èœè°±URL
        logger.info("ç¬¬ä¸€é˜¶æ®µ: çˆ¬å–åˆ—è¡¨é¡µï¼Œæ”¶é›†èœè°±é“¾æ¥...")
        recipe_urls = []

        for category_url in category_urls:
            # çˆ¬å–åˆ—è¡¨é¡µï¼ˆå¯èƒ½éœ€è¦æ»šåŠ¨åŠ è½½æ›´å¤šï¼‰
            html = await self.fetch_page(
                category_url,
                wait_selector='div.recipe-item',  # ç­‰å¾…åˆ—è¡¨é¡¹åŠ è½½
                scroll_count=3                     # æ»šåŠ¨3æ¬¡åŠ è½½æ›´å¤š
            )

            if html:
                urls = await self.parse_list_page(html)
                recipe_urls.extend(urls)

                # è¾¾åˆ°æ•°é‡ä¸Šé™åˆ™åœæ­¢
                if len(recipe_urls) >= max_recipes:
                    break

        logger.info(f"æ”¶é›†åˆ° {len(recipe_urls)} ä¸ªèœè°±é“¾æ¥")

        # ç¬¬äºŒé˜¶æ®µ: çˆ¬å–è¯¦æƒ…é¡µ
        logger.info("ç¬¬äºŒé˜¶æ®µ: çˆ¬å–è¯¦æƒ…é¡µï¼Œæå–èœè°±æ•°æ®...")

        for i, url in enumerate(recipe_urls[:max_recipes], 1):
            logger.info(f"æ­£åœ¨çˆ¬å– ({i}/{min(len(recipe_urls), max_recipes)}): {url}")

            # çˆ¬å–è¯¦æƒ…é¡µ
            html = await self.fetch_page(
                url,
                wait_selector='div.recipe-content',  # ç­‰å¾…å†…å®¹åŠ è½½
                scroll_count=2,                       # æ»šåŠ¨åŠ è½½å®Œæ•´æ­¥éª¤å›¾
                click_selectors=[                     # ç‚¹å‡»å±•å¼€æŒ‰é’®
                    '//button[contains(text(), "å±•å¼€å…¨éƒ¨æ­¥éª¤")]',
                    '//a[contains(text(), "æŸ¥çœ‹æ›´å¤š")]'
                ]
            )

            if html:
                recipes = await self.parse(html, url)
                all_recipes.extend(recipes)
                self.stats["items_scraped"] += len(recipes)

                # å¯é€‰: ä¿å­˜åˆ°æ•°æ®åº“
                if save_to_db:
                    for recipe in recipes:
                        self.save_to_db(recipe)

        self.end_stats()
        return all_recipes

    def save_to_db(self, recipe: Dict):
        """ä¿å­˜åˆ°æ•°æ®åº“ï¼ˆç¤ºä¾‹ï¼‰"""
        # TODO: å®ç°æ•°æ®åº“ä¿å­˜é€»è¾‘
        # å¯ä»¥ç”¨MongoDBã€MySQLç­‰
        pass

# ä½¿ç”¨ç¤ºä¾‹
async def main():
    # åŠ è½½ä»£ç†æ± 
    proxy_pool = ProxyPool.from_file("proxies.txt")

    # åˆ›å»ºçˆ¬è™«å®ä¾‹
    crawler = MeishitianxiaCrawler(
        proxy_pool=proxy_pool,
        headless=True
    )

    # ä½¿ç”¨async withè‡ªåŠ¨ç®¡ç†æµè§ˆå™¨
    async with crawler:
        # æ‰§è¡Œçˆ¬å–
        recipes = await crawler.run(
            category_urls=[
                "https://www.meishitianxia.com/chuancai/",
                "https://www.meishitianxia.com/yuecai/",
            ],
            max_recipes=50
        )

        # ä¿å­˜ç»“æœ
        import json
        with open("meishitianxia_recipes.json", "w", encoding="utf-8") as f:
            json.dump(recipes, f, ensure_ascii=False, indent=2)

        print(f"æˆåŠŸçˆ¬å– {len(recipes)} ä¸ªèœè°±")

if __name__ == "__main__":
    import asyncio
    asyncio.run(main())
```

---

## æ•°æ®éªŒè¯ä¸æ¸…æ´—

### ä½¿ç”¨DataValidator

```python
from gustobot.crawler.data_validator import DataValidator, RecipeModel

# éªŒè¯å•ä¸ªèœè°±
recipe_data = {
    "name": "çº¢çƒ§è‚‰",
    "ingredients": ["äº”èŠ±è‚‰500g", "å†°ç³–30g"],
    "steps": ["1. åˆ‡å—", "2. ç„¯æ°´", "3. ç‚–ç…®"]
}

validated = DataValidator.validate(recipe_data)
if validated:
    print(f"éªŒè¯æˆåŠŸ: {validated.name}")
    print(f"é£Ÿææ•°é‡: {len(validated.ingredients)}")

# æ‰¹é‡éªŒè¯
recipes = [recipe1, recipe2, recipe3]
valid_recipes = DataValidator.validate_batch(recipes)
print(f"æœ‰æ•ˆèœè°±: {len(valid_recipes)}/{len(recipes)}")

# å»é‡
unique_recipes = DataValidator.deduplicate(valid_recipes)
print(f"å»é‡å: {len(unique_recipes)} ä¸ªèœè°±")

# æ—¶é—´è§„èŒƒåŒ–
minutes = DataValidator.normalize_time("1å°æ—¶30åˆ†é’Ÿ")  # è¿”å› 90
minutes = DataValidator.normalize_time("PT1H30M")      # è¿”å› 90
```

---

## æœ€ä½³å®è·µ

### 1. åˆç†è®¾ç½®è¯·æ±‚å»¶è¿Ÿ

```python
# âŒ é”™è¯¯: å»¶è¿Ÿå¤ªçŸ­ï¼Œå®¹æ˜“è¢«å°
crawler = BrowserCrawler(request_delay=(0.1, 0.5))

# âœ… æ­£ç¡®: 2-5ç§’éšæœºå»¶è¿Ÿï¼Œæ¨¡æ‹Ÿäººç±»è¡Œä¸º
crawler = BrowserCrawler(request_delay=(2, 5))

# âœ… æ›´ä¿å®ˆ: å¯¹äºä¸¥æ ¼çš„ç½‘ç«™ï¼Œä½¿ç”¨æ›´é•¿å»¶è¿Ÿ
crawler = BrowserCrawler(request_delay=(5, 10))
```

### 2. ä½¿ç”¨ä»£ç†æ± 

```python
# âœ… æ¨è: ä½¿ç”¨ä»£ç†æ± é¿å…IPå°ç¦
proxy_pool = ProxyPool.from_file("proxies.txt")
crawler = BrowserCrawler(proxy_pool=proxy_pool)
```

### 3. éµå®ˆRobots.txt

```python
# âœ… é»˜è®¤å¼€å¯robots.txtæ£€æŸ¥
crawler = BaseCrawler(respect_robots_txt=True)  # é»˜è®¤å€¼

# âŒ ä¸æ¨è: é™¤éç¡®å®éœ€è¦ï¼Œå¦åˆ™ä¸è¦ç¦ç”¨
crawler = BaseCrawler(respect_robots_txt=False)
```

### 4. æ§åˆ¶å¹¶å‘

```python
import asyncio

async def crawl_with_limit(urls, max_concurrent=3):
    """é™åˆ¶å¹¶å‘æ•°é‡"""
    semaphore = asyncio.Semaphore(max_concurrent)

    async def fetch_one(url):
        async with semaphore:
            return await crawler.fetch_page(url)

    tasks = [fetch_one(url) for url in urls]
    return await asyncio.gather(*tasks)
```

### 5. é”™è¯¯å¤„ç†

```python
from loguru import logger

async def safe_crawl(url):
    try:
        html = await crawler.fetch_page(url)
        if html:
            return await crawler.parse(html, url)
    except Exception as e:
        logger.error(f"çˆ¬å–å¤±è´¥ {url}: {e}")
        return None
```

### 6. ä½¿ç”¨ä¸Šä¸‹æ–‡ç®¡ç†å™¨

```python
# âœ… æ¨è: ä½¿ç”¨async withè‡ªåŠ¨æ¸…ç†èµ„æº
async with BrowserCrawler() as crawler:
    recipes = await crawler.run(urls)

# âŒ ä¸æ¨è: æ‰‹åŠ¨ç®¡ç†ï¼Œå®¹æ˜“å¿˜è®°å…³é—­
crawler = BrowserCrawler()
await crawler.init_browser()
recipes = await crawler.run(urls)
await crawler.close_browser()  # å¯èƒ½å¿˜è®°è°ƒç”¨
```

---

## å¸¸è§é—®é¢˜

### Q1: å¦‚ä½•åˆ¤æ–­åº”è¯¥ç”¨HTTPçˆ¬è™«è¿˜æ˜¯æµè§ˆå™¨çˆ¬è™«ï¼Ÿ

**ç®€å•æµ‹è¯•**:
1. åœ¨æµè§ˆå™¨ä¸­æ‰“å¼€ç›®æ ‡é¡µé¢
2. å³é”® -> "æŸ¥çœ‹é¡µé¢æºä»£ç "
3. åœ¨æºä»£ç ä¸­æœç´¢ä½ æƒ³æå–çš„å†…å®¹

- âœ… **èƒ½æ‰¾åˆ°** -> ç”¨HTTPçˆ¬è™«ï¼ˆBaseCrawlerï¼‰
- âŒ **æ‰¾ä¸åˆ°** -> ç”¨æµè§ˆå™¨çˆ¬è™«ï¼ˆBrowserCrawlerï¼‰

### Q2: çˆ¬è™«ä¸€ç›´è¿”å›ç©ºæ•°æ®æ€ä¹ˆåŠï¼Ÿ

å¯èƒ½åŸå› :
1. **ç½‘ç«™éœ€è¦JavaScriptæ¸²æŸ“** -> æ”¹ç”¨BrowserCrawler
2. **é€‰æ‹©å™¨é”™è¯¯** -> æ£€æŸ¥XPath/CSSé€‰æ‹©å™¨
3. **è¢«åçˆ¬æ‹¦æˆª** -> æ£€æŸ¥å“åº”å†…å®¹ï¼Œæ·»åŠ ä»£ç†å’Œå»¶è¿Ÿ
4. **éœ€è¦ç™»å½•** -> æä¾›cookieså‚æ•°

è°ƒè¯•æ–¹æ³•:
```python
# ä¿å­˜HTMLæŸ¥çœ‹å†…å®¹
html = await crawler.fetch_page(url)
with open("debug.html", "w", encoding="utf-8") as f:
    f.write(html)

# æˆ–ä½¿ç”¨æˆªå›¾åŠŸèƒ½
await crawler.screenshot(url, "debug.png")
```

### Q3: å¦‚ä½•å¤„ç†éœ€è¦ç™»å½•çš„ç½‘ç«™ï¼Ÿ

```python
# æ–¹æ³•1: æä¾›Cookie
cookies = [
    {
        "name": "session_id",
        "value": "your_session_id",
        "domain": ".example.com",
        "path": "/"
    }
]

crawler = BrowserCrawler(cookies=cookies)

# æ–¹æ³•2: åœ¨æµè§ˆå™¨ä¸­æ‰‹åŠ¨ç™»å½•åä¿å­˜Cookie
# ï¼ˆå»ºè®®å…ˆç”¨headless=Falseæ‰‹åŠ¨ç™»å½•ä¸€æ¬¡ï¼‰
```

### Q4: çˆ¬è™«é€Ÿåº¦å¤ªæ…¢æ€ä¹ˆåŠï¼Ÿ

ä¼˜åŒ–æ–¹æ³•:
1. **å‡å°‘å»¶è¿Ÿ** (æ³¨æ„å¯èƒ½è¢«å°): `request_delay=(1, 2)`
2. **ä½¿ç”¨HTTPçˆ¬è™«** ä»£æ›¿æµè§ˆå™¨çˆ¬è™«ï¼ˆå¦‚æœå¯èƒ½ï¼‰
3. **å¹¶å‘çˆ¬å–** (æ§åˆ¶å¥½å¹¶å‘æ•°)
4. **å‡å°‘æ»šåŠ¨æ¬¡æ•°**: `scroll_count=1`
5. **è·³è¿‡ä¸å¿…è¦çš„ç­‰å¾…**: ä¸è®¾ç½®`wait_selector`

### Q5: å¦‚ä½•å¤„ç†åˆ†é¡µï¼Ÿ

```python
async def crawl_paginated_site(base_url, max_pages=10):
    recipes = []

    for page in range(1, max_pages + 1):
        # æ„é€ åˆ†é¡µURL
        url = f"{base_url}?page={page}"
        # æˆ–: url = f"{base_url}&start={(page-1)*15}"

        html = await crawler.fetch_page(url)
        if html:
            page_recipes = await crawler.parse(html, url)
            recipes.extend(page_recipes)

            # å¦‚æœé¡µé¢æ²¡æœ‰æ•°æ®ï¼Œè¯´æ˜åˆ°åº•äº†
            if not page_recipes:
                break

    return recipes
```

### Q6: ä»£ç†ç»å¸¸å¤±æ•ˆæ€ä¹ˆåŠï¼Ÿ

```python
# å¯åŠ¨ä»£ç†æ± å¥åº·æ£€æŸ¥
import asyncio

proxy_pool = ProxyPool.from_file("proxies.txt")

# åå°æŒç»­å¥åº·æ£€æŸ¥
asyncio.create_task(proxy_pool.start_health_check_loop())

# æˆ–å®šæœŸæ‰‹åŠ¨æ£€æŸ¥
await proxy_pool.health_check()
```

---

## ç›¸å…³æ–‡æ¡£

- [çˆ¬è™«ç¤ºä¾‹](crawler_examples.md) - æ›´å¤šå®æˆ˜æ¡ˆä¾‹
- [åçˆ¬è™«æœ€ä½³å®è·µ](anti_scraping_guide.md) - æ·±å…¥åçˆ¬æŠ€å·§
- [gustobot/crawler/README.md](../app/crawler/README.md) - æ¨¡å—æ–‡æ¡£

---

**ç¥ä½ çˆ¬å–é¡ºåˆ©ï¼** ğŸ•·ï¸

å¦‚æœ‰é—®é¢˜ï¼Œè¯·æäº¤Issue: https://github.com/yourusername/GustoBot/issues
