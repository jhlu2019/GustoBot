# GustoBot çˆ¬è™«å®æˆ˜ç¤ºä¾‹

æœ¬æ–‡æ¡£æä¾›äº†å¤šä¸ªå®æˆ˜çˆ¬è™«ç¤ºä¾‹ï¼Œæ¶µç›–ä¸åŒåœºæ™¯å’Œç½‘ç«™ç±»å‹ã€‚

## ğŸ“š ç›®å½•

- [ç¤ºä¾‹1: ä¸‹å¨æˆ¿ç½‘ç«™çˆ¬è™«](#ç¤ºä¾‹1-ä¸‹å¨æˆ¿ç½‘ç«™çˆ¬è™«)
- [ç¤ºä¾‹2: è±†æœç¾é£Ÿçˆ¬è™«](#ç¤ºä¾‹2-è±†æœç¾é£Ÿçˆ¬è™«)
- [ç¤ºä¾‹3: ç¾é£Ÿæ°çˆ¬è™«](#ç¤ºä¾‹3-ç¾é£Ÿæ°çˆ¬è™«)
- [ç¤ºä¾‹4: Schema.orgæ ‡å‡†ç½‘ç«™](#ç¤ºä¾‹4-schemaorgæ ‡å‡†ç½‘ç«™)
- [ç¤ºä¾‹5: ä¸¤é˜¶æ®µçˆ¬å–æ¨¡å¼](#ç¤ºä¾‹5-ä¸¤é˜¶æ®µçˆ¬å–æ¨¡å¼)
- [ç¤ºä¾‹6: MongoDBé›†æˆ](#ç¤ºä¾‹6-mongodbé›†æˆ)
- [ç¤ºä¾‹7: æ‰¹é‡çˆ¬å–ä¸å»é‡](#ç¤ºä¾‹7-æ‰¹é‡çˆ¬å–ä¸å»é‡)
- [ç¤ºä¾‹8: ä¸‹è½½å›¾ç‰‡](#ç¤ºä¾‹8-ä¸‹è½½å›¾ç‰‡)

---

## ç¤ºä¾‹1: ä¸‹å¨æˆ¿ç½‘ç«™çˆ¬è™«

ä¸‹å¨æˆ¿æ˜¯å…¸å‹çš„éœ€è¦JavaScriptæ¸²æŸ“çš„ç½‘ç«™ï¼Œéœ€è¦ä½¿ç”¨æµè§ˆå™¨çˆ¬è™«ã€‚

```python
"""
ä¸‹å¨æˆ¿èœè°±çˆ¬è™«
ç½‘ç«™: https://www.xiachufang.com/
ç‰¹ç‚¹:
- éœ€è¦æ»šåŠ¨åŠ è½½å®Œæ•´æ­¥éª¤
- é£Ÿæå’Œæ­¥éª¤åˆ†åˆ«åœ¨ä¸åŒçš„åŒºåŸŸ
- æœ‰ç”¨æˆ·è¯„è®ºåŒºéœ€è¦ç‚¹å‡»"å±•å¼€"
"""
from gustobot.crawler.browser_crawler import BrowserCrawler
from gustobot.crawler.proxy_pool import ProxyPool
from lxml import etree
from typing import List, Dict
import json


class XiachufangCrawler(BrowserCrawler):
    """ä¸‹å¨æˆ¿çˆ¬è™«"""

    def __init__(self, **kwargs):
        super().__init__(
            name="XiachufangCrawler",
            headless=True,
            request_delay=(2, 4),
            max_retries=3,
            **kwargs
        )

    async def parse(self, html_content: str, url: str) -> List[Dict]:
        """è§£æèœè°±è¯¦æƒ…é¡µ"""
        tree = etree.HTML(html_content)

        try:
            # åŸºæœ¬ä¿¡æ¯
            name = tree.xpath('//h1[@class="page-title"]/text()')[0].strip()

            # é£Ÿæï¼ˆåˆ†ä¸ºä¸»æ–™å’Œè¾…æ–™ï¼‰
            ingredients = []
            # ä¸»æ–™
            main_ingredients = tree.xpath('//div[@class="ingredients"]//tr')
            for ing in main_ingredients:
                name_elem = ing.xpath('.//td[@class="name"]/text()')
                amount_elem = ing.xpath('.//td[@class="unit"]/text()')
                if name_elem and amount_elem:
                    ingredients.append(f"{name_elem[0].strip()} {amount_elem[0].strip()}")

            # æ­¥éª¤
            steps = []
            step_elements = tree.xpath('//div[@class="steps"]//li[@class="step"]')
            for i, step in enumerate(step_elements, 1):
                text = step.xpath('.//p[@class="text"]/text()')
                img = step.xpath('.//img/@src')
                steps.append({
                    "step": i,
                    "description": text[0].strip() if text else "",
                    "image": img[0] if img else ""
                })

            # å°è´´å£«
            tips = tree.xpath('//div[@class="tip"]//p/text()')
            tips_text = tips[0].strip() if tips else ""

            # éš¾åº¦ã€æ—¶é—´ã€äººä»½
            stats = tree.xpath('//div[@class="recipe-stats"]//span/text()')
            difficulty = stats[0].strip() if len(stats) > 0 else ""
            time = stats[1].strip() if len(stats) > 1 else ""
            servings = stats[2].strip() if len(stats) > 2 else ""

            recipe = {
                "name": name,
                "ingredients": ingredients,
                "steps": steps,
                "tips": tips_text,
                "difficulty": difficulty,
                "time": time,
                "servings": servings,
                "url": url,
                "source": "ä¸‹å¨æˆ¿"
            }

            return [recipe]

        except Exception as e:
            self.logger.error(f"è§£æå¤±è´¥ {url}: {e}")
            return []

    async def run(self, urls: List[str]) -> List[Dict]:
        """æ‰§è¡Œçˆ¬å–"""
        self.start_stats()
        recipes = []

        for url in urls:
            html = await self.fetch_page(
                url,
                wait_selector='div.recipe-show',  # ç­‰å¾…ä¸»å†…å®¹åŠ è½½
                scroll_count=2,                    # æ»šåŠ¨åŠ è½½å›¾ç‰‡
                click_selectors=[                  # ç‚¹å‡»"å±•å¼€æ›´å¤š"
                    '//a[contains(text(), "å±•å¼€å…¨éƒ¨")]'
                ]
            )

            if html:
                parsed = await self.parse(html, url)
                recipes.extend(parsed)
                self.stats["items_scraped"] += len(parsed)

        self.end_stats()
        return recipes


# ä½¿ç”¨ç¤ºä¾‹
async def main():
    proxy_pool = ProxyPool.from_file("proxies.txt")
    crawler = XiachufangCrawler(proxy_pool=proxy_pool)

    async with crawler:
        recipes = await crawler.run([
            "https://www.xiachufang.com/recipe/12345/",
            "https://www.xiachufang.com/recipe/67890/"
        ])

        # ä¿å­˜ç»“æœ
        with open("xiachufang_recipes.json", "w", encoding="utf-8") as f:
            json.dump(recipes, f, ensure_ascii=False, indent=2)

        print(f"çˆ¬å–æˆåŠŸ: {len(recipes)} ä¸ªèœè°±")


if __name__ == "__main__":
    import asyncio
    asyncio.run(main())
```

---

## ç¤ºä¾‹2: è±†æœç¾é£Ÿçˆ¬è™«

è±†æœç¾é£Ÿæœ‰è¾ƒå¼ºçš„åçˆ¬æœºåˆ¶ï¼Œéœ€è¦ä½¿ç”¨ä»£ç†å’Œåˆç†çš„å»¶è¿Ÿã€‚

```python
"""
è±†æœç¾é£Ÿçˆ¬è™«
ç½‘ç«™: https://www.douguo.com/
ç‰¹ç‚¹:
- æœ‰åçˆ¬æœºåˆ¶ï¼Œéœ€è¦ä»£ç†
- å›¾ç‰‡æ‡’åŠ è½½ï¼Œéœ€è¦æ»šåŠ¨
- è¯„è®ºåŒºéœ€è¦ç‚¹å‡»"æŸ¥çœ‹æ›´å¤š"
"""
from gustobot.crawler.browser_crawler import BrowserCrawler
from lxml import etree
import re


class DouguoCrawler(BrowserCrawler):
    """è±†æœç¾é£Ÿçˆ¬è™«"""

    def __init__(self, **kwargs):
        super().__init__(
            name="DouguoCrawler",
            headless=True,
            request_delay=(3, 6),  # è¾ƒé•¿å»¶è¿Ÿé¿å…è¢«å°
            max_retries=3,
            **kwargs
        )

    async def parse(self, html_content: str, url: str) -> List[Dict]:
        """è§£æèœè°±é¡µé¢"""
        tree = etree.HTML(html_content)

        try:
            # èœè°±åç§°
            name = tree.xpath('//h1[@class="recipe_name"]/text()')[0].strip()

            # é£Ÿæ
            ingredients = []
            ing_items = tree.xpath('//div[@class="foodstuff"]//li')
            for item in ing_items:
                name_elem = item.xpath('.//a/text()')
                weight_elem = item.xpath('.//span/text()')
                if name_elem:
                    ing_name = name_elem[0].strip()
                    weight = weight_elem[0].strip() if weight_elem else ""
                    ingredients.append(f"{ing_name} {weight}".strip())

            # æ­¥éª¤
            steps = []
            step_items = tree.xpath('//div[@class="steps"]//div[@class="step"]')
            for i, step in enumerate(step_items, 1):
                desc = step.xpath('.//div[@class="text"]/text()')
                img = step.xpath('.//img/@data-src | .//img/@src')
                steps.append({
                    "step": i,
                    "description": desc[0].strip() if desc else "",
                    "image": img[0] if img else ""
                })

            # å°è´´å£«
            tips = tree.xpath('//div[@class="recipe_tips"]//p/text()')

            # ç»Ÿè®¡ä¿¡æ¯
            cook_time = tree.xpath('//span[@class="cook_time"]/text()')
            difficulty = tree.xpath('//span[@class="difficulty"]/text()')

            recipe = {
                "name": name,
                "ingredients": ingredients,
                "steps": steps,
                "tips": "\n".join([t.strip() for t in tips]),
                "time": cook_time[0].strip() if cook_time else "",
                "difficulty": difficulty[0].strip() if difficulty else "",
                "url": url,
                "source": "è±†æœç¾é£Ÿ"
            }

            return [recipe]

        except Exception as e:
            self.logger.error(f"è§£æå¤±è´¥: {e}")
            return []

    async def search_recipes(self, keyword: str, max_results: int = 10) -> List[str]:
        """æœç´¢èœè°±å¹¶è¿”å›URLåˆ—è¡¨"""
        search_url = f"https://www.douguo.com/search/recipe/{keyword}"

        html = await self.fetch_page(
            search_url,
            wait_selector='div.recipe_list',
            scroll_count=3
        )

        if not html:
            return []

        tree = etree.HTML(html)
        links = tree.xpath('//div[@class="recipe_item"]//a[@class="recipe_name"]/@href')

        # è¡¥å…¨URLå¹¶å»é‡
        full_urls = list(set([
            f"https://www.douguo.com{link}" if link.startswith('/') else link
            for link in links[:max_results]
        ]))

        self.logger.info(f"æœç´¢åˆ° {len(full_urls)} ä¸ªèœè°±")
        return full_urls

    async def run(self, keywords: List[str], max_per_keyword: int = 5):
        """è¿è¡Œçˆ¬è™«"""
        self.start_stats()
        all_recipes = []

        for keyword in keywords:
            self.logger.info(f"æ­£åœ¨æœç´¢: {keyword}")

            # ç¬¬ä¸€æ­¥: æœç´¢è·å–URL
            urls = await self.search_recipes(keyword, max_per_keyword)

            # ç¬¬äºŒæ­¥: çˆ¬å–è¯¦æƒ…é¡µ
            for url in urls:
                html = await self.fetch_page(
                    url,
                    wait_selector='div.recipe_content',
                    scroll_count=2
                )

                if html:
                    recipes = await self.parse(html, url)
                    all_recipes.extend(recipes)
                    self.stats["items_scraped"] += len(recipes)

        self.end_stats()
        return all_recipes


# ä½¿ç”¨ç¤ºä¾‹
async def main():
    from gustobot.crawler.proxy_pool import ProxyPool

    proxy_pool = ProxyPool.from_file("proxies.txt")
    crawler = DouguoCrawler(proxy_pool=proxy_pool)

    async with crawler:
        recipes = await crawler.run(
            keywords=["çº¢çƒ§è‚‰", "ç³–é†‹æ’éª¨", "éº»å©†è±†è…"],
            max_per_keyword=5
        )

        print(f"å…±çˆ¬å– {len(recipes)} ä¸ªèœè°±")
```

---

## ç¤ºä¾‹3: ç¾é£Ÿæ°çˆ¬è™«

ä½¿ç”¨HTTPçˆ¬è™«çš„ç¤ºä¾‹ï¼ˆé™æ€é¡µé¢ï¼‰ã€‚

```python
"""
ç¾é£Ÿæ°çˆ¬è™« (HTTPç‰ˆæœ¬)
ç½‘ç«™: https://www.meishij.net/
ç‰¹ç‚¹:
- é™æ€HTMLï¼Œä¸éœ€è¦æµè§ˆå™¨
- ä½¿ç”¨HTTPçˆ¬è™«å³å¯ï¼Œé€Ÿåº¦å¿«
"""
from gustobot.crawler import BaseCrawler
import httpx
from bs4 import BeautifulSoup
from typing import List, Dict


class MeishijCrawler(BaseCrawler):
    """ç¾é£Ÿæ°çˆ¬è™«ï¼ˆHTTPç‰ˆæœ¬ï¼‰"""

    def __init__(self, **kwargs):
        super().__init__(
            name="MeishijCrawler",
            request_delay=(1, 3),
            max_retries=3,
            **kwargs
        )

    async def parse(self, response: httpx.Response) -> Dict:
        """è§£æå“åº”"""
        soup = BeautifulSoup(response.text, 'html.parser')

        try:
            # èœè°±åç§°
            name = soup.find('h1', class_='recipe_title').text.strip()

            # é£Ÿæ
            ingredients = []
            ing_list = soup.find('div', class_='materials')
            if ing_list:
                for li in ing_list.find_all('li'):
                    ingredients.append(li.text.strip())

            # æ­¥éª¤
            steps = []
            step_list = soup.find('div', class_='recipe_steps')
            if step_list:
                for i, step_div in enumerate(step_list.find_all('div', class_='step'), 1):
                    text = step_div.find('p', class_='text')
                    img = step_div.find('img')
                    steps.append({
                        "step": i,
                        "description": text.text.strip() if text else "",
                        "image": img['src'] if img else ""
                    })

            recipe = {
                "name": name,
                "ingredients": ingredients,
                "steps": steps,
                "url": str(response.url),
                "source": "ç¾é£Ÿæ°"
            }

            return recipe

        except Exception as e:
            self.logger.error(f"è§£æå¤±è´¥: {e}")
            return None

    async def run(self, urls: List[str]) -> List[Dict]:
        """è¿è¡Œçˆ¬è™«"""
        self.start_stats()
        recipes = []

        for url in urls:
            response = await self.fetch(url)
            if response:
                recipe = await self.parse(response)
                if recipe:
                    recipes.append(recipe)
                    self.stats["items_scraped"] += 1

        self.end_stats()
        return recipes


# ä½¿ç”¨ç¤ºä¾‹
async def main():
    crawler = MeishijCrawler()

    recipes = await crawler.run([
        "https://www.meishij.net/recipe/12345.html",
        "https://www.meishij.net/recipe/67890.html"
    ])

    print(f"çˆ¬å–æˆåŠŸ: {len(recipes)} ä¸ªèœè°±")
```

---

## ç¤ºä¾‹4: Schema.orgæ ‡å‡†ç½‘ç«™

ä½¿ç”¨å†…ç½®çš„RecipeCrawlerçˆ¬å–ç¬¦åˆSchema.orgæ ‡å‡†çš„ç½‘ç«™ã€‚

```python
"""
Schema.orgæ ‡å‡†ç½‘ç«™çˆ¬è™«
é€‚ç”¨äºæ‰€æœ‰å®ç°äº†Schema.org Recipeæ ‡å‡†çš„ç½‘ç«™
"""
from gustobot.crawler import RecipeCrawler
from gustobot.crawler.proxy_pool import ProxyPool


async def crawl_schema_org_sites():
    """çˆ¬å–Schema.orgæ ‡å‡†ç½‘ç«™"""

    # åˆ›å»ºçˆ¬è™«ï¼ˆè‡ªåŠ¨è¯†åˆ«JSON-LDå’ŒMicrodataï¼‰
    proxy_pool = ProxyPool.from_file("proxies.txt")
    crawler = RecipeCrawler(proxy_pool=proxy_pool)

    # çˆ¬å–å¤šä¸ªç½‘ç«™
    urls = [
        "https://www.allrecipes.com/recipe/12345/",
        "https://www.food.com/recipe/67890/",
        "https://cooking.nytimes.com/recipes/12345-recipe"
    ]

    recipes = await crawler.run(urls)

    # æ•°æ®éªŒè¯
    from gustobot.crawler.data_validator import DataValidator

    valid_recipes = DataValidator.validate_batch(recipes)
    unique_recipes = DataValidator.deduplicate(valid_recipes)

    print(f"çˆ¬å–: {len(recipes)} ä¸ª")
    print(f"æœ‰æ•ˆ: {len(valid_recipes)} ä¸ª")
    print(f"å»é‡: {len(unique_recipes)} ä¸ª")

    return unique_recipes


# è¿è¡Œ
if __name__ == "__main__":
    import asyncio
    asyncio.run(crawl_schema_org_sites())
```

---

## ç¤ºä¾‹5: ä¸¤é˜¶æ®µçˆ¬å–æ¨¡å¼

å…ˆçˆ¬åˆ—è¡¨é¡µè·å–é“¾æ¥ï¼Œå†çˆ¬è¯¦æƒ…é¡µè·å–æ•°æ®ã€‚

```python
"""
ä¸¤é˜¶æ®µçˆ¬å–æ¨¡å¼
Stage 1: åˆ—è¡¨é¡µ -> æ”¶é›†URL
Stage 2: è¯¦æƒ…é¡µ -> æå–æ•°æ®
"""
from gustobot.crawler.browser_crawler import BrowserCrawler
from lxml import etree
from typing import List


class TwoStageCrawler(BrowserCrawler):
    """ä¸¤é˜¶æ®µçˆ¬è™«"""

    async def crawl_list_page(self, list_url: str, max_pages: int = 5) -> List[str]:
        """
        ç¬¬ä¸€é˜¶æ®µ: çˆ¬å–åˆ—è¡¨é¡µï¼Œæ”¶é›†èœè°±URL

        Args:
            list_url: åˆ—è¡¨é¡µURLï¼ˆå¯èƒ½æ˜¯æœç´¢ç»“æœæˆ–åˆ†ç±»é¡µï¼‰
            max_pages: æœ€å¤§çˆ¬å–é¡µæ•°

        Returns:
            èœè°±URLåˆ—è¡¨
        """
        all_urls = []

        for page in range(1, max_pages + 1):
            # æ„é€ åˆ†é¡µURL
            paginated_url = f"{list_url}?page={page}"
            self.logger.info(f"æ­£åœ¨çˆ¬å–åˆ—è¡¨é¡µ {page}/{max_pages}")

            html = await self.fetch_page(
                paginated_url,
                wait_selector='div.recipe-list',
                scroll_count=3  # æ»šåŠ¨åŠ è½½æ›´å¤š
            )

            if not html:
                break

            # æå–é“¾æ¥
            tree = etree.HTML(html)
            links = tree.xpath('//div[@class="recipe-item"]//a/@href')

            if not links:
                self.logger.info("æ²¡æœ‰æ›´å¤šé“¾æ¥ï¼Œåœæ­¢çˆ¬å–")
                break

            # è¡¥å…¨URL
            full_links = [
                f"https://example.com{link}" if link.startswith('/') else link
                for link in links
            ]

            all_urls.extend(full_links)
            self.logger.info(f"ä»ç¬¬{page}é¡µæå–äº† {len(full_links)} ä¸ªé“¾æ¥")

        # å»é‡
        unique_urls = list(set(all_urls))
        self.logger.info(f"æ€»å…±æ”¶é›†åˆ° {len(unique_urls)} ä¸ªå”¯ä¸€é“¾æ¥")

        return unique_urls

    async def crawl_detail_page(self, url: str):
        """
        ç¬¬äºŒé˜¶æ®µ: çˆ¬å–è¯¦æƒ…é¡µï¼Œæå–èœè°±æ•°æ®
        """
        html = await self.fetch_page(
            url,
            wait_selector='div.recipe-content',
            scroll_count=2,
            click_selectors=['//button[contains(text(), "å±•å¼€")]']
        )

        if html:
            return await self.parse(html, url)
        return None

    async def parse(self, html_content: str, url: str):
        """è§£æè¯¦æƒ…é¡µ"""
        # å®ç°è§£æé€»è¾‘
        pass

    async def run(self, list_url: str, max_pages: int = 5, max_recipes: int = 50):
        """
        å®Œæ•´çš„ä¸¤é˜¶æ®µçˆ¬å–æµç¨‹

        Args:
            list_url: åˆ—è¡¨é¡µURL
            max_pages: æœ€å¤§åˆ—è¡¨é¡µæ•°
            max_recipes: æœ€å¤§èœè°±æ•°
        """
        self.start_stats()

        # ç¬¬ä¸€é˜¶æ®µ: æ”¶é›†URL
        self.logger.info("=" * 60)
        self.logger.info("ç¬¬ä¸€é˜¶æ®µ: çˆ¬å–åˆ—è¡¨é¡µï¼Œæ”¶é›†èœè°±é“¾æ¥")
        self.logger.info("=" * 60)

        recipe_urls = await self.crawl_list_page(list_url, max_pages)

        # ç¬¬äºŒé˜¶æ®µ: çˆ¬å–è¯¦æƒ…
        self.logger.info("=" * 60)
        self.logger.info("ç¬¬äºŒé˜¶æ®µ: çˆ¬å–è¯¦æƒ…é¡µï¼Œæå–èœè°±æ•°æ®")
        self.logger.info("=" * 60)

        recipes = []
        for i, url in enumerate(recipe_urls[:max_recipes], 1):
            self.logger.info(f"æ­£åœ¨çˆ¬å– ({i}/{min(len(recipe_urls), max_recipes)}): {url}")

            recipe = await self.crawl_detail_page(url)
            if recipe:
                recipes.append(recipe)
                self.stats["items_scraped"] += 1

        self.end_stats()
        return recipes


# ä½¿ç”¨ç¤ºä¾‹
async def main():
    from gustobot.crawler.proxy_pool import ProxyPool

    proxy_pool = ProxyPool.from_file("proxies.txt")
    crawler = TwoStageCrawler(proxy_pool=proxy_pool, headless=True)

    async with crawler:
        recipes = await crawler.run(
            list_url="https://example.com/recipes",
            max_pages=5,
            max_recipes=50
        )

        print(f"æˆåŠŸçˆ¬å– {len(recipes)} ä¸ªèœè°±")


if __name__ == "__main__":
    import asyncio
    asyncio.run(main())
```

---

## ç¤ºä¾‹6: MongoDBé›†æˆ

å°†çˆ¬å–çš„æ•°æ®ç›´æ¥ä¿å­˜åˆ°MongoDBã€‚

```python
"""
MongoDBé›†æˆç¤ºä¾‹
å®æ—¶ä¿å­˜çˆ¬å–ç»“æœï¼Œé¿å…æ•°æ®ä¸¢å¤±
"""
from gustobot.crawler.browser_crawler import BrowserCrawler
from pymongo import MongoClient
from datetime import datetime


class MongoRecipeCrawler(BrowserCrawler):
    """MongoDBé›†æˆçˆ¬è™«"""

    def __init__(
        self,
        mongo_uri: str = "mongodb://localhost:27017/",
        db_name: str = "recipe_db",
        collection_name: str = "recipes",
        **kwargs
    ):
        super().__init__(name="MongoRecipeCrawler", **kwargs)

        # è¿æ¥MongoDB
        self.client = MongoClient(mongo_uri)
        self.db = self.client[db_name]
        self.collection = self.db[collection_name]

        # åˆ›å»ºå”¯ä¸€ç´¢å¼•ï¼ˆé¿å…é‡å¤ï¼‰
        self.collection.create_index("url", unique=True)

        self.logger.info(f"å·²è¿æ¥MongoDB: {mongo_uri} / {db_name}.{collection_name}")

    def save_recipe(self, recipe: dict) -> bool:
        """
        ä¿å­˜èœè°±åˆ°MongoDB

        Returns:
            True: ä¿å­˜æˆåŠŸ
            False: å·²å­˜åœ¨ï¼Œè·³è¿‡
        """
        try:
            # æ·»åŠ æ—¶é—´æˆ³
            recipe["crawled_at"] = datetime.now()
            recipe["updated_at"] = datetime.now()

            # æ’å…¥ï¼ˆå¦‚æœURLå·²å­˜åœ¨ä¼šæŠ›å‡ºå¼‚å¸¸ï¼‰
            self.collection.insert_one(recipe)
            self.logger.info(f"âœ… å·²ä¿å­˜: {recipe['name']}")
            return True

        except Exception as e:
            if "duplicate key error" in str(e):
                self.logger.info(f"â­ï¸  å·²å­˜åœ¨ï¼Œè·³è¿‡: {recipe.get('name', 'Unknown')}")
            else:
                self.logger.error(f"âŒ ä¿å­˜å¤±è´¥: {e}")
            return False

    def recipe_exists(self, url: str) -> bool:
        """æ£€æŸ¥èœè°±æ˜¯å¦å·²å­˜åœ¨"""
        return self.collection.find_one({"url": url}) is not None

    async def parse(self, html_content: str, url: str):
        """è§£æé¡µé¢"""
        # å®ç°è§£æé€»è¾‘
        recipe = {
            "name": "ç¤ºä¾‹èœè°±",
            "url": url,
            "source": "ç¤ºä¾‹ç½‘ç«™"
        }
        return [recipe]

    async def run(self, urls: List[str], skip_existing: bool = True):
        """
        è¿è¡Œçˆ¬è™«

        Args:
            urls: URLåˆ—è¡¨
            skip_existing: æ˜¯å¦è·³è¿‡å·²å­˜åœ¨çš„URL
        """
        self.start_stats()

        for i, url in enumerate(urls, 1):
            # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
            if skip_existing and self.recipe_exists(url):
                self.logger.info(f"({i}/{len(urls)}) å·²å­˜åœ¨ï¼Œè·³è¿‡: {url}")
                continue

            self.logger.info(f"({i}/{len(urls)}) æ­£åœ¨çˆ¬å–: {url}")

            # çˆ¬å–é¡µé¢
            html = await self.fetch_page(url, scroll_count=2)
            if html:
                recipes = await self.parse(html, url)

                # ä¿å­˜åˆ°MongoDB
                for recipe in recipes:
                    if self.save_recipe(recipe):
                        self.stats["items_scraped"] += 1

        self.end_stats()

        # è¿”å›ç»Ÿè®¡ä¿¡æ¯
        total_count = self.collection.count_documents({})
        return {
            "crawled": self.stats["items_scraped"],
            "total_in_db": total_count
        }

    def close(self):
        """å…³é—­MongoDBè¿æ¥"""
        if self.client:
            self.client.close()
            self.logger.info("MongoDBè¿æ¥å·²å…³é—­")


# ä½¿ç”¨ç¤ºä¾‹
async def main():
    crawler = MongoRecipeCrawler(
        mongo_uri="mongodb://localhost:27017/",
        db_name="recipe_db",
        collection_name="recipes",
        headless=True
    )

    try:
        async with crawler:
            result = await crawler.run(
                urls=["https://example.com/recipe1", "https://example.com/recipe2"],
                skip_existing=True
            )

            print(f"æœ¬æ¬¡çˆ¬å–: {result['crawled']} ä¸ª")
            print(f"æ•°æ®åº“æ€»æ•°: {result['total_in_db']} ä¸ª")

    finally:
        crawler.close()


if __name__ == "__main__":
    import asyncio
    asyncio.run(main())
```

---

## ç¤ºä¾‹7: æ‰¹é‡çˆ¬å–ä¸å»é‡

å¤„ç†å¤§é‡URLå¹¶è‡ªåŠ¨å»é‡ã€‚

```python
"""
æ‰¹é‡çˆ¬å–ä¸å»é‡ç¤ºä¾‹
"""
import asyncio
from gustobot.crawler.browser_crawler import BrowserCrawler
from gustobot.crawler.data_validator import DataValidator
from typing import List, Set
import json


class BatchCrawler(BrowserCrawler):
    """æ‰¹é‡çˆ¬è™«"""

    def __init__(self, **kwargs):
        super().__init__(name="BatchCrawler", **kwargs)
        self.seen_urls: Set[str] = set()  # å·²çˆ¬å–çš„URL

    async def parse(self, html_content: str, url: str):
        """è§£æé¡µé¢"""
        # å®ç°è§£æé€»è¾‘
        pass

    async def crawl_batch(
        self,
        urls: List[str],
        batch_size: int = 10,
        concurrent: int = 3
    ) -> List[dict]:
        """
        æ‰¹é‡çˆ¬å–

        Args:
            urls: URLåˆ—è¡¨
            batch_size: æ¯æ‰¹å¤„ç†æ•°é‡
            concurrent: å¹¶å‘æ•°
        """
        all_recipes = []

        # åˆ†æ‰¹å¤„ç†
        for i in range(0, len(urls), batch_size):
            batch = urls[i:i + batch_size]
            self.logger.info(f"æ­£åœ¨å¤„ç†ç¬¬ {i//batch_size + 1} æ‰¹ ({len(batch)} ä¸ªURL)")

            # ä½¿ç”¨ä¿¡å·é‡æ§åˆ¶å¹¶å‘
            semaphore = asyncio.Semaphore(concurrent)

            async def fetch_one(url):
                # è·³è¿‡å·²çˆ¬å–çš„URL
                if url in self.seen_urls:
                    self.logger.info(f"è·³è¿‡é‡å¤URL: {url}")
                    return None

                async with semaphore:
                    self.seen_urls.add(url)
                    html = await self.fetch_page(url, scroll_count=2)
                    if html:
                        return await self.parse(html, url)
                    return None

            # å¹¶å‘çˆ¬å–
            tasks = [fetch_one(url) for url in batch]
            results = await asyncio.gather(*tasks, return_exceptions=True)

            # æ”¶é›†ç»“æœ
            for result in results:
                if result and not isinstance(result, Exception):
                    all_recipes.extend(result)

            self.logger.info(f"ç¬¬ {i//batch_size + 1} æ‰¹å®Œæˆï¼Œå·²çˆ¬å– {len(all_recipes)} ä¸ªèœè°±")

        return all_recipes

    async def run(self, urls: List[str]) -> List[dict]:
        """è¿è¡Œçˆ¬è™«"""
        self.start_stats()

        # æ‰¹é‡çˆ¬å–
        recipes = await self.crawl_batch(urls, batch_size=10, concurrent=3)

        # æ•°æ®éªŒè¯å’Œå»é‡
        self.logger.info("æ­£åœ¨éªŒè¯å’Œå»é‡...")
        valid_recipes = DataValidator.validate_batch(recipes)
        unique_recipes = DataValidator.deduplicate(valid_recipes)

        self.stats["items_scraped"] = len(unique_recipes)
        self.end_stats()

        self.logger.info(f"åŸå§‹: {len(recipes)}, æœ‰æ•ˆ: {len(valid_recipes)}, å»é‡: {len(unique_recipes)}")

        return [r.dict() for r in unique_recipes]


# ä½¿ç”¨ç¤ºä¾‹
async def main():
    # ä»æ–‡ä»¶åŠ è½½URLåˆ—è¡¨
    with open("urls.txt", "r") as f:
        urls = [line.strip() for line in f if line.strip()]

    crawler = BatchCrawler(headless=True)

    async with crawler:
        recipes = await crawler.run(urls)

        # ä¿å­˜ç»“æœ
        with open("recipes_batch.json", "w", encoding="utf-8") as f:
            json.dump(recipes, f, ensure_ascii=False, indent=2)

        print(f"æˆåŠŸçˆ¬å–å¹¶å»é‡: {len(recipes)} ä¸ªèœè°±")


if __name__ == "__main__":
    asyncio.run(main())
```

---

## ç¤ºä¾‹8: ä¸‹è½½å›¾ç‰‡

çˆ¬å–èœè°±å¹¶ä¸‹è½½ç›¸å…³å›¾ç‰‡ã€‚

```python
"""
ä¸‹è½½å›¾ç‰‡ç¤ºä¾‹
"""
from gustobot.crawler.browser_crawler import BrowserCrawler
import httpx
import os
from pathlib import Path
import hashlib
from urllib.parse import urlparse


class ImageDownloadCrawler(BrowserCrawler):
    """å¸¦å›¾ç‰‡ä¸‹è½½åŠŸèƒ½çš„çˆ¬è™«"""

    def __init__(self, image_dir: str = "images", **kwargs):
        super().__init__(name="ImageDownloadCrawler", **kwargs)
        self.image_dir = Path(image_dir)
        self.image_dir.mkdir(exist_ok=True)

    async def download_image(self, url: str, filename: str = None) -> str:
        """
        ä¸‹è½½å›¾ç‰‡

        Args:
            url: å›¾ç‰‡URL
            filename: ä¿å­˜æ–‡ä»¶åï¼ˆå¯é€‰ï¼Œé»˜è®¤ä½¿ç”¨URLå“ˆå¸Œï¼‰

        Returns:
            æœ¬åœ°æ–‡ä»¶è·¯å¾„
        """
        if not url:
            return ""

        try:
            # ç”Ÿæˆæ–‡ä»¶å
            if not filename:
                url_hash = hashlib.md5(url.encode()).hexdigest()
                ext = Path(urlparse(url).path).suffix or '.jpg'
                filename = f"{url_hash}{ext}"

            filepath = self.image_dir / filename

            # å¦‚æœæ–‡ä»¶å·²å­˜åœ¨ï¼Œè·³è¿‡ä¸‹è½½
            if filepath.exists():
                self.logger.debug(f"å›¾ç‰‡å·²å­˜åœ¨: {filepath}")
                return str(filepath)

            # ä¸‹è½½å›¾ç‰‡
            async with httpx.AsyncClient() as client:
                response = await client.get(url, timeout=30)
                response.raise_for_status()

                # ä¿å­˜æ–‡ä»¶
                with open(filepath, 'wb') as f:
                    f.write(response.content)

                self.logger.info(f"âœ… å·²ä¸‹è½½: {filepath}")
                return str(filepath)

        except Exception as e:
            self.logger.error(f"ä¸‹è½½å›¾ç‰‡å¤±è´¥ {url}: {e}")
            return ""

    async def parse(self, html_content: str, url: str):
        """è§£æé¡µé¢å¹¶ä¸‹è½½å›¾ç‰‡"""
        from lxml import etree

        tree = etree.HTML(html_content)

        # æå–åŸºæœ¬ä¿¡æ¯
        name = tree.xpath('//h1[@class="title"]/text()')[0].strip()

        # ä¸»å›¾
        main_image_url = tree.xpath('//img[@class="main-image"]/@src')[0]
        main_image_path = await self.download_image(main_image_url, f"{name}_main.jpg")

        # æ­¥éª¤å›¾
        steps = []
        step_elements = tree.xpath('//div[@class="steps"]//div[@class="step"]')
        for i, step in enumerate(step_elements, 1):
            desc = step.xpath('.//p/text()')[0].strip()
            img_url = step.xpath('.//img/@src')[0] if step.xpath('.//img/@src') else ""

            # ä¸‹è½½æ­¥éª¤å›¾
            img_path = await self.download_image(img_url, f"{name}_step{i}.jpg") if img_url else ""

            steps.append({
                "step": i,
                "description": desc,
                "image_url": img_url,
                "image_path": img_path
            })

        recipe = {
            "name": name,
            "main_image_url": main_image_url,
            "main_image_path": main_image_path,
            "steps": steps,
            "url": url
        }

        return [recipe]

    async def run(self, urls: List[str]):
        """è¿è¡Œçˆ¬è™«"""
        self.start_stats()
        recipes = []

        for url in urls:
            html = await self.fetch_page(url, scroll_count=2)
            if html:
                parsed = await self.parse(html, url)
                recipes.extend(parsed)
                self.stats["items_scraped"] += len(parsed)

        self.end_stats()

        self.logger.info(f"çˆ¬å–å®Œæˆï¼Œå›¾ç‰‡ä¿å­˜åœ¨: {self.image_dir}")
        return recipes


# ä½¿ç”¨ç¤ºä¾‹
async def main():
    crawler = ImageDownloadCrawler(
        image_dir="downloaded_images",
        headless=True
    )

    async with crawler:
        recipes = await crawler.run([
            "https://example.com/recipe1",
            "https://example.com/recipe2"
        ])

        # ç»Ÿè®¡
        total_images = sum(
            1 + len(r['steps'])  # ä¸»å›¾ + æ­¥éª¤å›¾
            for r in recipes
        )

        print(f"çˆ¬å–: {len(recipes)} ä¸ªèœè°±")
        print(f"ä¸‹è½½: {total_images} å¼ å›¾ç‰‡")


if __name__ == "__main__":
    import asyncio
    asyncio.run(main())
```

---

## æ€»ç»“

ä»¥ä¸Šç¤ºä¾‹æ¶µç›–äº†ï¼š

1. âœ… **ä¸åŒç½‘ç«™ç±»å‹**: åŠ¨æ€é¡µé¢ã€é™æ€é¡µé¢ã€Schema.orgæ ‡å‡†
2. âœ… **ä¸åŒçˆ¬å–æ¨¡å¼**: å•é¡µã€åˆ—è¡¨+è¯¦æƒ…ã€æœç´¢+è¯¦æƒ…
3. âœ… **æ•°æ®å­˜å‚¨**: JSONæ–‡ä»¶ã€MongoDB
4. âœ… **é«˜çº§åŠŸèƒ½**: æ‰¹é‡å¤„ç†ã€å¹¶å‘æ§åˆ¶ã€å›¾ç‰‡ä¸‹è½½
5. âœ… **åçˆ¬æœºåˆ¶**: ä»£ç†æ± ã€å»¶è¿Ÿã€é‡è¯•

ä½ å¯ä»¥æ ¹æ®ç›®æ ‡ç½‘ç«™çš„ç‰¹ç‚¹ï¼Œé€‰æ‹©åˆé€‚çš„ç¤ºä¾‹ä½œä¸ºèµ·ç‚¹ï¼Œè¿›è¡Œå®šåˆ¶åŒ–å¼€å‘ã€‚

---

**æ›´å¤šæ–‡æ¡£**:
- [çˆ¬è™«ä½¿ç”¨æŒ‡å—](crawler_guide.md)
- [åçˆ¬è™«æœ€ä½³å®è·µ](anti_scraping_guide.md)

ç¥ä½ çˆ¬å–æ„‰å¿«ï¼ ğŸ•·ï¸
