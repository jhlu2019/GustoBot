# åçˆ¬è™«æœ€ä½³å®è·µæŒ‡å—

æœ¬æŒ‡å—è¯¦ç»†ä»‹ç»å¦‚ä½•è§„é¿å¸¸è§çš„åçˆ¬è™«æœºåˆ¶,å®ç°ç¨³å®šã€é«˜æ•ˆã€åˆæ³•çš„æ•°æ®é‡‡é›†ã€‚

## ğŸ“š ç›®å½•

- [æ³•å¾‹ä¸é“å¾·å‡†åˆ™](#æ³•å¾‹ä¸é“å¾·å‡†åˆ™)
- [å¸¸è§åçˆ¬è™«æœºåˆ¶](#å¸¸è§åçˆ¬è™«æœºåˆ¶)
- [åçˆ¬è™«ç­–ç•¥](#åçˆ¬è™«ç­–ç•¥)
- [ä»£ç†æ± æœ€ä½³å®è·µ](#ä»£ç†æ± æœ€ä½³å®è·µ)
- [æµè§ˆå™¨æŒ‡çº¹å¯¹æŠ—](#æµè§ˆå™¨æŒ‡çº¹å¯¹æŠ—)
- [è¯·æ±‚é¢‘ç‡æ§åˆ¶](#è¯·æ±‚é¢‘ç‡æ§åˆ¶)
- [éªŒè¯ç å¤„ç†](#éªŒè¯ç å¤„ç†)
- [åŠ¨æ€å†…å®¹å¤„ç†](#åŠ¨æ€å†…å®¹å¤„ç†)
- [ç›‘æ§ä¸åº”æ€¥](#ç›‘æ§ä¸åº”æ€¥)
- [æ¡ˆä¾‹åˆ†æ](#æ¡ˆä¾‹åˆ†æ)

---

## æ³•å¾‹ä¸é“å¾·å‡†åˆ™

### âš–ï¸ æ³•å¾‹åˆè§„

åœ¨å¼€å§‹çˆ¬å–ä¹‹å‰ï¼Œè¯·ç¡®ä¿ï¼š

1. **éµå®ˆRobots.txt** âœ…
   - æ‰€æœ‰çˆ¬è™«é»˜è®¤éµå®ˆrobots.txtåè®®
   - ä¸çˆ¬å–è¢«ç¦æ­¢çš„è·¯å¾„

2. **éµå®ˆç½‘ç«™æœåŠ¡æ¡æ¬¾** âœ…
   - é˜…è¯»ç›®æ ‡ç½‘ç«™çš„æœåŠ¡æ¡æ¬¾
   - ä¸çˆ¬å–æ˜ç¡®ç¦æ­¢é‡‡é›†çš„å†…å®¹

3. **å°Šé‡ç‰ˆæƒ** âœ…
   - ä»…çˆ¬å–å…¬å¼€æ•°æ®
   - ä¸ç”¨äºå•†ä¸šç”¨é€”ï¼ˆé™¤éè·å¾—æˆæƒï¼‰
   - æ³¨æ˜æ•°æ®æ¥æº

4. **ä¸ªäººéšç§ä¿æŠ¤** âœ…
   - ä¸é‡‡é›†ä¸ªäººéšç§ä¿¡æ¯
   - éµå®ˆGDPRã€ä¸ªäººä¿¡æ¯ä¿æŠ¤æ³•ç­‰æ³•è§„

### ğŸ¤ é“å¾·å‡†åˆ™

1. **åˆç†çš„çˆ¬å–é¢‘ç‡**
   - ä¸å¯¹ç›®æ ‡æœåŠ¡å™¨é€ æˆè¿‡å¤§å‹åŠ›
   - é¿å¼€ç½‘ç«™é«˜å³°æ—¶æ®µ

2. **æ ‡è¯†èº«ä»½**
   - ä½¿ç”¨æè¿°æ€§çš„User-Agent
   - æä¾›è”ç³»æ–¹å¼ï¼ˆå¦‚æœ‰å¿…è¦ï¼‰

3. **å°Šé‡æœåŠ¡å™¨èµ„æº**
   - ä½¿ç”¨ç¼“å­˜ï¼Œé¿å…é‡å¤è¯·æ±‚
   - å®ç°å¢é‡çˆ¬å–

---

## å¸¸è§åçˆ¬è™«æœºåˆ¶

### 1. åŸºäºè¯·æ±‚å¤´çš„æ£€æµ‹

**æ£€æµ‹æ–¹æ³•**:
- æ£€æŸ¥User-Agent
- æ£€æŸ¥Referer
- æ£€æŸ¥Accept-Languageã€Accept-Encodingç­‰

**ç‰¹å¾**:
```
çˆ¬è™«: User-Agent: python-requests/2.31.0
æ­£å¸¸: User-Agent: Mozilla/5.0 (Windows NT 10.0...) Chrome/120.0.0.0
```

### 2. åŸºäºIPçš„é™åˆ¶

**é™åˆ¶æ–¹å¼**:
- å•IPè¯·æ±‚é¢‘ç‡é™åˆ¶
- IPé»‘åå•
- IPåœ°åŸŸé™åˆ¶

**ç°è±¡**:
- 429 Too Many Requests
- 403 Forbidden
- ç›´æ¥è¿”å›ç©ºæ•°æ®

### 3. åŸºäºCookie/Sessionçš„éªŒè¯

**æœºåˆ¶**:
- é¦–æ¬¡è®¿é—®è®¾ç½®Cookie
- åç»­è¯·æ±‚éœ€è¦å¸¦ä¸ŠCookie
- Sessionè¶…æ—¶æœºåˆ¶

### 4. JavaScriptæŒ‘æˆ˜

**ç±»å‹**:
- è®¡ç®—æŒ‘æˆ˜ï¼ˆevalã€å¤æ‚è¿ç®—ï¼‰
- æµè§ˆå™¨ç¯å¢ƒæ£€æµ‹
- CanvasæŒ‡çº¹
- WebGLæŒ‡çº¹

### 5. åŠ¨æ€åŠ å¯†å‚æ•°

**è¡¨ç°**:
- è¯·æ±‚å‚æ•°åŒ…å«ç­¾å: `sign=abc123`
- ç­¾åé€šè¿‡JavaScriptåŠ¨æ€ç”Ÿæˆ
- æ—¶é—´æˆ³å‚æ•°: `timestamp=1234567890`

### 6. éªŒè¯ç 

**ç±»å‹**:
- å›¾ç‰‡éªŒè¯ç 
- æ»‘åŠ¨éªŒè¯ç ï¼ˆæéªŒã€è…¾è®¯äº‘ç­‰ï¼‰
- ç‚¹å‡»éªŒè¯ç 
- reCAPTCHA

### 7. è¡Œä¸ºåˆ†æ

**ç›‘æ§æŒ‡æ ‡**:
- é¼ æ ‡è½¨è¿¹
- é”®ç›˜è¾“å…¥
- é¡µé¢åœç•™æ—¶é—´
- æ»šåŠ¨é€Ÿåº¦
- ç‚¹å‡»ä½ç½®

---

## åçˆ¬è™«ç­–ç•¥

### ç­–ç•¥1: User-Agentè½®æ¢

**é—®é¢˜**: é»˜è®¤User-Agentä¼šæš´éœ²çˆ¬è™«èº«ä»½

**è§£å†³æ–¹æ¡ˆ**:

```python
from server.crawler.browser_crawler import BrowserCrawler

# æ–¹æ³•1: ä½¿ç”¨å†…ç½®çš„éšæœºUser-Agentæ± 
crawler = BrowserCrawler(use_random_ua=True)  # é»˜è®¤å¼€å¯

# æ–¹æ³•2: è‡ªå®šä¹‰User-Agentæ± 
class CustomUACrawler(BrowserCrawler):
    USER_AGENTS = [
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) Chrome/120.0.0.0',
        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) Safari/605.1.15',
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:121.0) Firefox/121.0',
        # æ·»åŠ æ›´å¤š...
    ]

# æ–¹æ³•3: ä½¿ç”¨fake-useragentåº“ï¼ˆåŠ¨æ€è·å–çœŸå®UAï¼‰
from fake_useragent import UserAgent
ua = UserAgent()

crawler = BrowserCrawler(
    extra_headers={"User-Agent": ua.random}
)
```

**æœ€ä½³å®è·µ**:
- âœ… ä½¿ç”¨çœŸå®æµè§ˆå™¨çš„User-Agent
- âœ… å®šæœŸæ›´æ–°User-Agentæ± 
- âŒ ä¸ä½¿ç”¨æ˜æ˜¾çš„çˆ¬è™«UAï¼ˆå¦‚python-requestsï¼‰

### ç­–ç•¥2: IPä»£ç†æ± 

**é—®é¢˜**: å•ä¸ªIPé¢‘ç¹è¯·æ±‚è¢«å°ç¦

**è§£å†³æ–¹æ¡ˆ**:

```python
from server.crawler.proxy_pool import ProxyPool
from server.crawler.browser_crawler import BrowserCrawler

# åˆ›å»ºä»£ç†æ± 
proxy_pool = ProxyPool(
    check_interval=300,    # 5åˆ†é’Ÿå¥åº·æ£€æŸ¥ä¸€æ¬¡
    max_fail_count=5,      # å¤±è´¥5æ¬¡åç¦ç”¨ä»£ç†
    timeout=10.0           # ä»£ç†æµ‹è¯•è¶…æ—¶10ç§’
)

# ä»æ–‡ä»¶åŠ è½½ä»£ç†
proxy_pool = ProxyPool.from_file("proxies.txt")

# ä½¿ç”¨ä»£ç†æ± 
crawler = BrowserCrawler(proxy_pool=proxy_pool)
```

**ä»£ç†ç±»å‹é€‰æ‹©**:

1. **æ•°æ®ä¸­å¿ƒä»£ç†** (Datacenter Proxies)
   - ä¼˜ç‚¹: ä¾¿å®œã€é€Ÿåº¦å¿«
   - ç¼ºç‚¹: å®¹æ˜“è¢«è¯†åˆ«å’Œå°ç¦
   - é€‚ç”¨: ä¸ä¸¥æ ¼çš„ç½‘ç«™

2. **ä½å®…ä»£ç†** (Residential Proxies)
   - ä¼˜ç‚¹: çœŸå®IPï¼Œä¸æ˜“è¢«å°
   - ç¼ºç‚¹: è´µã€é€Ÿåº¦æ…¢
   - é€‚ç”¨: ä¸¥æ ¼çš„ç½‘ç«™

3. **ç§»åŠ¨ä»£ç†** (Mobile Proxies)
   - ä¼˜ç‚¹: æ›´çœŸå®ï¼Œå¾ˆå°‘è¢«å°
   - ç¼ºç‚¹: æœ€è´µ
   - é€‚ç”¨: éå¸¸ä¸¥æ ¼çš„ç½‘ç«™

**ä»£ç†è·å–æ¸ é“**:
- ä»˜è´¹ä»£ç†æœåŠ¡: Bright Dataã€Oxylabsã€Smartproxy
- å…è´¹ä»£ç†: ä¸æ¨èï¼ˆä¸ç¨³å®šã€ä¸å®‰å…¨ï¼‰
- è‡ªå»ºä»£ç†æ± : éœ€è¦æŠ€æœ¯æŠ•å…¥

### ç­–ç•¥3: è¯·æ±‚å»¶è¿Ÿ

**é—®é¢˜**: è¯·æ±‚è¿‡å¿«è¢«è¯†åˆ«ä¸ºæœºå™¨äºº

**è§£å†³æ–¹æ¡ˆ**:

```python
# æ–¹æ³•1: å›ºå®šå»¶è¿Ÿï¼ˆä¸æ¨èï¼‰
import asyncio
await asyncio.sleep(2)

# æ–¹æ³•2: éšæœºå»¶è¿Ÿï¼ˆæ¨èï¼‰
crawler = BrowserCrawler(
    request_delay=(2, 5)  # 2-5ç§’éšæœºå»¶è¿Ÿ
)

# æ–¹æ³•3: æŒ‡æ•°é€€é¿ï¼ˆé‡è¯•æ—¶ä½¿ç”¨ï¼‰
async def fetch_with_backoff(url, max_retries=3):
    for attempt in range(max_retries):
        try:
            return await crawler.fetch_page(url)
        except Exception as e:
            if attempt < max_retries - 1:
                wait_time = 2 ** attempt  # 1s, 2s, 4s, 8s...
                await asyncio.sleep(wait_time)
            else:
                raise
```

**å»¶è¿Ÿè®¾ç½®å»ºè®®**:

| ç½‘ç«™ç±»å‹ | å»¶è¿ŸèŒƒå›´ | è¯´æ˜ |
|---------|---------|-----|
| å®½æ¾ | 1-2ç§’ | æ— æ˜æ˜¾é™åˆ¶ |
| ä¸€èˆ¬ | 2-5ç§’ | æœ‰åŸºç¡€é™åˆ¶ |
| ä¸¥æ ¼ | 5-10ç§’ | ä¸¥æ ¼çš„é¢‘ç‡æ§åˆ¶ |
| æä¸¥æ ¼ | 10-30ç§’ | æç«¯åçˆ¬æœºåˆ¶ |

### ç­–ç•¥4: Cookieç®¡ç†

**é—®é¢˜**: ç½‘ç«™éœ€è¦Cookieæ‰èƒ½è®¿é—®

**è§£å†³æ–¹æ¡ˆ**:

```python
# æ–¹æ³•1: æä¾›åˆå§‹Cookie
cookies = [
    {
        "name": "session_id",
        "value": "abc123",
        "domain": ".example.com",
        "path": "/",
        "httpOnly": True,
        "secure": True
    }
]

crawler = BrowserCrawler(cookies=cookies)

# æ–¹æ³•2: è‡ªåŠ¨è·å–Cookieï¼ˆæ¨¡æ‹Ÿç™»å½•ï¼‰
async def login_and_get_cookies():
    crawler = BrowserCrawler(headless=False)  # æ˜¾ç¤ºæµè§ˆå™¨
    await crawler.init_browser()

    page = await crawler.new_page()

    # è®¿é—®ç™»å½•é¡µ
    await page.goto("https://example.com/login")

    # å¡«å†™è¡¨å•
    await page.fill('input[name="username"]', "your_username")
    await page.fill('input[name="password"]', "your_password")

    # ç‚¹å‡»ç™»å½•æŒ‰é’®
    await page.click('button[type="submit"]')

    # ç­‰å¾…ç™»å½•æˆåŠŸ
    await page.wait_for_selector('div.user-profile')

    # è·å–Cookie
    cookies = await crawler.context.cookies()

    # ä¿å­˜Cookie
    import json
    with open("cookies.json", "w") as f:
        json.dump(cookies, f)

    await crawler.close_browser()
    return cookies

# æ–¹æ³•3: ä»æ–‡ä»¶åŠ è½½Cookie
import json
with open("cookies.json", "r") as f:
    cookies = json.load(f)

crawler = BrowserCrawler(cookies=cookies)
```

### ç­–ç•¥5: æ¨¡æ‹Ÿäººç±»è¡Œä¸º

**é—®é¢˜**: è¡Œä¸ºç‰¹å¾åƒæœºå™¨äºº

**è§£å†³æ–¹æ¡ˆ**:

```python
async def human_like_crawl(crawler, url):
    """æ¨¡æ‹Ÿäººç±»æµè§ˆè¡Œä¸º"""

    page = await crawler.new_page()

    # 1. è®¿é—®é¡µé¢
    await page.goto(url)

    # 2. éšæœºç­‰å¾…ï¼ˆè¯»å–æ—¶é—´ï¼‰
    await page.wait_for_timeout(random.randint(1000, 3000))

    # 3. æ¨¡æ‹Ÿé¼ æ ‡ç§»åŠ¨
    await page.mouse.move(random.randint(100, 500), random.randint(100, 500))
    await page.wait_for_timeout(random.randint(500, 1000))

    # 4. æ¨¡æ‹Ÿæ»šåŠ¨ï¼ˆé€æ­¥æ»šåŠ¨ï¼Œä¸æ˜¯ä¸€æ¬¡æ€§ï¼‰
    for _ in range(3):
        scroll_amount = random.randint(300, 800)
        await page.mouse.wheel(0, scroll_amount)
        await page.wait_for_timeout(random.randint(1000, 2000))

    # 5. æ¨¡æ‹Ÿç‚¹å‡»ï¼ˆå¦‚æœéœ€è¦ï¼‰
    try:
        # éšæœºç‚¹å‡»æŸä¸ªå…ƒç´ 
        element = await page.query_selector('a')
        if element:
            await element.click()
            await page.wait_for_timeout(random.randint(500, 1500))
            await page.go_back()
    except:
        pass

    # 6. è·å–å†…å®¹
    content = await page.content()
    await page.close()

    return content
```

**è¡Œä¸ºæ¨¡æ‹Ÿæ¸…å•**:
- âœ… éšæœºåœç•™æ—¶é—´
- âœ… æ¸è¿›å¼æ»šåŠ¨
- âœ… é¼ æ ‡ç§»åŠ¨è½¨è¿¹
- âœ… éšæœºç‚¹å‡»
- âœ… è¿”å›ä¸Šä¸€é¡µ
- âœ… è®¿é—®ç›¸å…³é¡µé¢

### ç­–ç•¥6: Refererä¼ªé€ 

**é—®é¢˜**: ç½‘ç«™æ£€æŸ¥Refereråˆ¤æ–­è¯·æ±‚æ¥æº

**è§£å†³æ–¹æ¡ˆ**:

```python
# HTTPçˆ¬è™«
async def fetch_with_referer(url):
    headers = {
        "Referer": "https://example.com/",  # ä¼ªé€ æ¥æº
        "User-Agent": "Mozilla/5.0 ..."
    }
    response = await crawler.fetch(url, headers=headers)

# æµè§ˆå™¨çˆ¬è™«
crawler = BrowserCrawler(
    extra_headers={
        "Referer": "https://example.com/"
    }
)
```

---

## ä»£ç†æ± æœ€ä½³å®è·µ

### 1. ä»£ç†è´¨é‡è¯„ä¼°

```python
from server.crawler.proxy_pool import ProxyPool
import asyncio

async def evaluate_proxies():
    proxy_pool = ProxyPool.from_file("proxies.txt")

    # å¥åº·æ£€æŸ¥
    await proxy_pool.health_check()

    # æŸ¥çœ‹ç»Ÿè®¡
    stats = proxy_pool.get_stats()

    print(f"æ€»ä»£ç†æ•°: {stats['total_proxies']}")
    print(f"æ´»è·ƒä»£ç†: {stats['active_proxies']}")
    print(f"å¹³å‡æˆåŠŸç‡: {stats['average_success_rate']:.2%}")
    print(f"å¹³å‡å“åº”æ—¶é—´: {stats['average_response_time']:.2f}s")

    # ç­›é€‰ä¼˜è´¨ä»£ç†
    good_proxies = [
        p for p in proxy_pool.proxies
        if p.is_active and p.success_rate > 0.8 and p.response_time < 5.0
    ]

    print(f"ä¼˜è´¨ä»£ç†: {len(good_proxies)} ä¸ª")
```

### 2. ä»£ç†è½®æ¢ç­–ç•¥

```python
# ç­–ç•¥1: åŠ æƒéšæœºï¼ˆå†…ç½®ï¼‰
# æˆåŠŸç‡é«˜çš„ä»£ç†æœ‰æ›´é«˜æ¦‚ç‡è¢«é€‰ä¸­
proxy_pool = ProxyPool()  # é»˜è®¤ä½¿ç”¨åŠ æƒéšæœº

# ç­–ç•¥2: è½®è¯¢
class RoundRobinProxyPool(ProxyPool):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.current_index = 0

    async def get_proxy(self):
        if not self.proxies:
            return None

        active_proxies = [p for p in self.proxies if p.is_active]
        if not active_proxies:
            return None

        proxy = active_proxies[self.current_index % len(active_proxies)]
        self.current_index += 1

        return proxy.to_dict()

# ç­–ç•¥3: ç²˜æ€§ä»£ç†ï¼ˆåŒä¸€ä¼šè¯ä½¿ç”¨åŒä¸€ä»£ç†ï¼‰
class StickyProxyPool(ProxyPool):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.session_proxies = {}

    async def get_proxy_for_session(self, session_id: str):
        if session_id in self.session_proxies:
            return self.session_proxies[session_id]

        proxy = await self.get_proxy()
        self.session_proxies[session_id] = proxy
        return proxy
```

### 3. ä»£ç†è‡ªåŠ¨æ¢å¤

```python
import asyncio
from server.crawler.proxy_pool import ProxyPool

async def auto_recovery_pool():
    """è‡ªåŠ¨æ¢å¤å¤±æ•ˆçš„ä»£ç†"""
    proxy_pool = ProxyPool.from_file("proxies.txt")

    # å¯åŠ¨å¥åº·æ£€æŸ¥å¾ªç¯
    async def health_check_loop():
        while True:
            await proxy_pool.health_check()
            await asyncio.sleep(300)  # æ¯5åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡

    # å¯åŠ¨æ¢å¤å¾ªç¯
    async def recovery_loop():
        while True:
            # å°è¯•æ¢å¤å¤±è´¥æ¬¡æ•°å¤šçš„ä»£ç†
            for proxy in proxy_pool.proxies:
                if not proxy.is_active and proxy.fail_count >= proxy_pool.max_fail_count:
                    # é‡ç½®å¤±è´¥è®¡æ•°ï¼Œç»™äºˆé‡æ–°å°è¯•çš„æœºä¼š
                    proxy.fail_count = 0
                    proxy.is_active = True
                    logger.info(f"å°è¯•æ¢å¤ä»£ç†: {proxy.host}:{proxy.port}")

            await asyncio.sleep(1800)  # æ¯30åˆ†é’Ÿå°è¯•æ¢å¤

    # åŒæ—¶è¿è¡Œä¸¤ä¸ªå¾ªç¯
    await asyncio.gather(
        health_check_loop(),
        recovery_loop()
    )
```

---

## æµè§ˆå™¨æŒ‡çº¹å¯¹æŠ—

### 1. ä¿®æ”¹æµè§ˆå™¨æŒ‡çº¹

```python
from playwright.async_api import async_playwright

async def stealth_browser():
    """éšèº«æµè§ˆå™¨é…ç½®"""
    playwright = await async_playwright().start()

    browser = await playwright.chromium.launch(
        headless=True,
        args=[
            '--disable-blink-features=AutomationControlled',  # ç¦ç”¨è‡ªåŠ¨åŒ–æ ‡è¯†
            '--disable-dev-shm-usage',
            '--no-sandbox',
            '--disable-setuid-sandbox',
            '--disable-web-security',
        ]
    )

    context = await browser.new_context(
        viewport={'width': 1920, 'height': 1080},
        user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
        locale='zh-CN',
        timezone_id='Asia/Shanghai',
        permissions=['geolocation'],
        geolocation={'latitude': 39.9042, 'longitude': 116.4074},  # åŒ—äº¬
    )

    # æ³¨å…¥è„šæœ¬éšè—webdriveræ ‡è¯†
    await context.add_init_script("""
        Object.defineProperty(navigator, 'webdriver', {
            get: () => undefined
        });

        // éšè—Chromeæ ‡è¯†
        window.navigator.chrome = {
            runtime: {}
        };

        // ä¼ªé€ pluginæ•°é‡
        Object.defineProperty(navigator, 'plugins', {
            get: () => [1, 2, 3, 4, 5]
        });

        // ä¼ªé€ è¯­è¨€
        Object.defineProperty(navigator, 'languages', {
            get: () => ['zh-CN', 'zh', 'en-US', 'en']
        });
    """)

    page = await context.new_page()
    return page
```

### 2. CanvasæŒ‡çº¹å¯¹æŠ—

```python
async def anti_canvas_fingerprint(page):
    """å¯¹æŠ—CanvasæŒ‡çº¹"""
    await page.add_init_script("""
        const originalToDataURL = HTMLCanvasElement.prototype.toDataURL;
        HTMLCanvasElement.prototype.toDataURL = function(type) {
            // åœ¨canvasæ•°æ®ä¸Šæ·»åŠ è½»å¾®å™ªå£°
            const context = this.getContext('2d');
            const imageData = context.getImageData(0, 0, this.width, this.height);
            for (let i = 0; i < imageData.data.length; i += 4) {
                imageData.data[i] += Math.floor(Math.random() * 10) - 5;
            }
            context.putImageData(imageData, 0, 0);
            return originalToDataURL.apply(this, arguments);
        };
    """)
```

---

## è¯·æ±‚é¢‘ç‡æ§åˆ¶

### 1. ä»¤ç‰Œæ¡¶ç®—æ³•

```python
import asyncio
import time

class TokenBucket:
    """ä»¤ç‰Œæ¡¶é™æµå™¨"""

    def __init__(self, rate: float, capacity: int):
        """
        Args:
            rate: æ¯ç§’ç”Ÿæˆçš„ä»¤ç‰Œæ•°
            capacity: æ¡¶å®¹é‡
        """
        self.rate = rate
        self.capacity = capacity
        self.tokens = capacity
        self.last_update = time.time()
        self._lock = asyncio.Lock()

    async def acquire(self, tokens: int = 1) -> bool:
        """è·å–ä»¤ç‰Œ"""
        async with self._lock:
            now = time.time()
            # æ·»åŠ æ–°ä»¤ç‰Œ
            self.tokens = min(
                self.capacity,
                self.tokens + (now - self.last_update) * self.rate
            )
            self.last_update = now

            if self.tokens >= tokens:
                self.tokens -= tokens
                return True
            return False

    async def wait_for_token(self, tokens: int = 1):
        """ç­‰å¾…ç›´åˆ°è·å–åˆ°ä»¤ç‰Œ"""
        while not await self.acquire(tokens):
            await asyncio.sleep(0.1)


# ä½¿ç”¨ç¤ºä¾‹
async def rate_limited_crawl():
    # æ¯ç§’æœ€å¤š2ä¸ªè¯·æ±‚ï¼Œæ¡¶å®¹é‡10
    limiter = TokenBucket(rate=2.0, capacity=10)

    urls = [f"https://example.com/page{i}" for i in range(100)]

    for url in urls:
        await limiter.wait_for_token()  # ç­‰å¾…ä»¤ç‰Œ
        response = await crawler.fetch_page(url)
        # å¤„ç†å“åº”...
```

### 2. æ»‘åŠ¨çª—å£ç®—æ³•

```python
from collections import deque
import time

class SlidingWindowLimiter:
    """æ»‘åŠ¨çª—å£é™æµå™¨"""

    def __init__(self, max_requests: int, window_size: int):
        """
        Args:
            max_requests: çª—å£å†…æœ€å¤§è¯·æ±‚æ•°
            window_size: çª—å£å¤§å°ï¼ˆç§’ï¼‰
        """
        self.max_requests = max_requests
        self.window_size = window_size
        self.requests = deque()

    async def acquire(self) -> bool:
        """å°è¯•è·å–è¯·æ±‚è®¸å¯"""
        now = time.time()

        # ç§»é™¤çª—å£å¤–çš„è¯·æ±‚
        while self.requests and self.requests[0] < now - self.window_size:
            self.requests.popleft()

        # æ£€æŸ¥æ˜¯å¦è¶…è¿‡é™åˆ¶
        if len(self.requests) < self.max_requests:
            self.requests.append(now)
            return True

        return False

    async def wait_for_slot(self):
        """ç­‰å¾…ç›´åˆ°æœ‰å¯ç”¨æ§½ä½"""
        while not await self.acquire():
            await asyncio.sleep(0.1)


# ä½¿ç”¨ç¤ºä¾‹
async def crawl_with_limit():
    # 60ç§’å†…æœ€å¤š100ä¸ªè¯·æ±‚
    limiter = SlidingWindowLimiter(max_requests=100, window_size=60)

    for url in urls:
        await limiter.wait_for_slot()
        response = await crawler.fetch_page(url)
```

---

## éªŒè¯ç å¤„ç†

### 1. å›¾ç‰‡éªŒè¯ç 

```python
# æ–¹æ³•1: æ‰‹åŠ¨å¤„ç†ï¼ˆå¼€å‘é˜¶æ®µï¼‰
async def manual_captcha(page):
    """æ‰‹åŠ¨è¾“å…¥éªŒè¯ç """
    # ç­‰å¾…éªŒè¯ç å›¾ç‰‡åŠ è½½
    await page.wait_for_selector('img.captcha')

    # æˆªå›¾ä¿å­˜éªŒè¯ç 
    captcha_element = await page.query_selector('img.captcha')
    await captcha_element.screenshot(path='captcha.png')

    # æ‰‹åŠ¨è¾“å…¥
    captcha_code = input("è¯·è¾“å…¥éªŒè¯ç : ")

    # å¡«å†™éªŒè¯ç 
    await page.fill('input[name="captcha"]', captcha_code)

# æ–¹æ³•2: OCRè‡ªåŠ¨è¯†åˆ«ï¼ˆç®€å•éªŒè¯ç ï¼‰
async def ocr_captcha(page):
    """ä½¿ç”¨OCRè¯†åˆ«éªŒè¯ç """
    import pytesseract
    from PIL import Image

    # æˆªå›¾
    captcha_element = await page.query_selector('img.captcha')
    await captcha_element.screenshot(path='captcha.png')

    # OCRè¯†åˆ«
    image = Image.open('captcha.png')
    captcha_code = pytesseract.image_to_string(image)

    # å¡«å†™
    await page.fill('input[name="captcha"]', captcha_code.strip())

# æ–¹æ³•3: ç¬¬ä¸‰æ–¹æ‰“ç å¹³å°
async def third_party_captcha(page, api_key):
    """ä½¿ç”¨æ‰“ç å¹³å°"""
    # 1. è·å–éªŒè¯ç å›¾ç‰‡
    captcha_url = await page.get_attribute('img.captcha', 'src')

    # 2. æäº¤åˆ°æ‰“ç å¹³å°ï¼ˆç¤ºä¾‹ï¼‰
    import httpx
    async with httpx.AsyncClient() as client:
        response = await client.post(
            'https://api.captcha-service.com/solve',
            json={'image_url': captcha_url, 'api_key': api_key}
        )
        result = response.json()
        captcha_code = result['code']

    # 3. å¡«å†™éªŒè¯ç 
    await page.fill('input[name="captcha"]', captcha_code)
```

### 2. æ»‘åŠ¨éªŒè¯ç 

```python
async def solve_slider_captcha(page):
    """è§£å†³æ»‘åŠ¨éªŒè¯ç """
    # ç­‰å¾…æ»‘å—å‡ºç°
    await page.wait_for_selector('div.slider-button')

    # è·å–æ»‘å—å’Œæ»‘è½¨å…ƒç´ 
    slider = await page.query_selector('div.slider-button')
    track = await page.query_selector('div.slider-track')

    # è·å–éœ€è¦æ»‘åŠ¨çš„è·ç¦»
    track_box = await track.bounding_box()
    slider_box = await slider.bounding_box()
    distance = track_box['width'] - slider_box['width']

    # æ¨¡æ‹Ÿäººç±»æ»‘åŠ¨ï¼ˆä¸æ˜¯åŒ€é€Ÿï¼‰
    await slider.hover()
    await page.mouse.down()

    # åˆ†æ®µæ»‘åŠ¨ï¼Œæ¨¡æ‹ŸåŠ é€Ÿå‡é€Ÿ
    steps = []
    current = 0

    # å¿«é€Ÿé˜¶æ®µ
    while current < distance * 0.6:
        step = random.randint(5, 15)
        steps.append(step)
        current += step

    # å‡é€Ÿé˜¶æ®µ
    while current < distance * 0.95:
        step = random.randint(2, 5)
        steps.append(step)
        current += step

    # ç²¾ç¡®è°ƒæ•´
    steps.append(distance - current)

    # æ‰§è¡Œæ»‘åŠ¨
    for step in steps:
        await page.mouse.move(
            slider_box['x'] + current,
            slider_box['y'],
            steps=random.randint(5, 10)
        )
        current += step
        await asyncio.sleep(random.uniform(0.01, 0.05))

    await page.mouse.up()
```

---

## åŠ¨æ€å†…å®¹å¤„ç†

### 1. AJAXæ•°æ®åŠ è½½

```python
async def wait_for_ajax(page, timeout=30000):
    """ç­‰å¾…AJAXè¯·æ±‚å®Œæˆ"""
    # æ–¹æ³•1: ç­‰å¾…ç‰¹å®šå…ƒç´ å‡ºç°
    await page.wait_for_selector('div.data-loaded', timeout=timeout)

    # æ–¹æ³•2: ç­‰å¾…ç½‘ç»œç©ºé—²
    await page.wait_for_load_state('networkidle', timeout=timeout)

    # æ–¹æ³•3: ç­‰å¾…ç‰¹å®šAPIè¯·æ±‚
    async with page.expect_response(
        lambda response: 'api/data' in response.url
    ) as response_info:
        await page.click('button.load-more')
    response = await response_info.value
    data = await response.json()
```

### 2. æ— é™æ»šåŠ¨

```python
async def scroll_to_load_all(page, max_scrolls=10):
    """æ»šåŠ¨åŠ è½½æ‰€æœ‰å†…å®¹"""
    previous_height = 0

    for i in range(max_scrolls):
        # æ»šåŠ¨åˆ°åº•éƒ¨
        await page.evaluate('window.scrollTo(0, document.body.scrollHeight)')

        # ç­‰å¾…åŠ è½½
        await asyncio.sleep(2)

        # æ£€æŸ¥æ˜¯å¦è¿˜æœ‰æ–°å†…å®¹
        current_height = await page.evaluate('document.body.scrollHeight')

        if current_height == previous_height:
            # æ²¡æœ‰æ–°å†…å®¹ï¼Œåœæ­¢æ»šåŠ¨
            break

        previous_height = current_height
        print(f"æ»šåŠ¨ç¬¬ {i+1} æ¬¡ï¼Œé«˜åº¦: {current_height}")

    print(f"æ»šåŠ¨å®Œæˆï¼Œå…±æ»šåŠ¨ {i+1} æ¬¡")
```

### 3. æ‡’åŠ è½½å›¾ç‰‡

```python
async def load_lazy_images(page):
    """åŠ è½½æ‰€æœ‰æ‡’åŠ è½½å›¾ç‰‡"""
    # æ»šåŠ¨è§¦å‘æ‡’åŠ è½½
    await page.evaluate("""
        async () => {
            const scrollHeight = document.body.scrollHeight;
            const step = 500;

            for (let y = 0; y < scrollHeight; y += step) {
                window.scrollTo(0, y);
                await new Promise(resolve => setTimeout(resolve, 100));
            }
        }
    """)

    # ç­‰å¾…æ‰€æœ‰å›¾ç‰‡åŠ è½½å®Œæˆ
    await page.wait_for_load_state('networkidle')
```

---

## ç›‘æ§ä¸åº”æ€¥

### 1. ç›‘æ§çˆ¬è™«çŠ¶æ€

```python
from dataclasses import dataclass
from datetime import datetime
import json

@dataclass
class CrawlerMetrics:
    """çˆ¬è™«æŒ‡æ ‡"""
    start_time: datetime
    requests_sent: int = 0
    requests_failed: int = 0
    items_scraped: int = 0
    proxies_banned: int = 0
    captchas_encountered: int = 0

    @property
    def success_rate(self):
        total = self.requests_sent
        return (total - self.requests_failed) / total if total > 0 else 0

    @property
    def runtime_seconds(self):
        return (datetime.now() - self.start_time).total_seconds()

    def report(self):
        """ç”ŸæˆæŠ¥å‘Š"""
        return {
            "è¿è¡Œæ—¶é•¿": f"{self.runtime_seconds:.0f}ç§’",
            "è¯·æ±‚æ€»æ•°": self.requests_sent,
            "æˆåŠŸæ•°": self.requests_sent - self.requests_failed,
            "å¤±è´¥æ•°": self.requests_failed,
            "æˆåŠŸç‡": f"{self.success_rate:.2%}",
            "çˆ¬å–æ¡ç›®": self.items_scraped,
            "ä»£ç†è¢«å°": self.proxies_banned,
            "é‡åˆ°éªŒè¯ç ": self.captchas_encountered
        }


# ä½¿ç”¨ç¤ºä¾‹
class MonitoredCrawler(BrowserCrawler):
    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        self.metrics = CrawlerMetrics(start_time=datetime.now())

    async def fetch_page(self, url, **kwargs):
        self.metrics.requests_sent += 1

        try:
            result = await super().fetch_page(url, **kwargs)
            return result
        except Exception as e:
            self.metrics.requests_failed += 1
            if "403" in str(e) or "429" in str(e):
                self.metrics.proxies_banned += 1
            raise

    def print_report(self):
        report = self.metrics.report()
        print("\n" + "="*50)
        print("çˆ¬è™«è¿è¡ŒæŠ¥å‘Š")
        print("="*50)
        for key, value in report.items():
            print(f"{key}: {value}")
        print("="*50 + "\n")
```

### 2. å¼‚å¸¸å¤„ç†ä¸é‡è¯•

```python
async def smart_retry(func, max_retries=3, backoff_factor=2):
    """æ™ºèƒ½é‡è¯•æœºåˆ¶"""
    for attempt in range(max_retries):
        try:
            return await func()
        except Exception as e:
            error_msg = str(e)

            # æ ¹æ®é”™è¯¯ç±»å‹å†³å®šæ˜¯å¦é‡è¯•
            if "429" in error_msg:  # é¢‘ç‡é™åˆ¶
                wait_time = backoff_factor ** attempt * 10
                logger.warning(f"è§¦å‘é¢‘ç‡é™åˆ¶ï¼Œç­‰å¾… {wait_time}ç§’åé‡è¯•...")
                await asyncio.sleep(wait_time)

            elif "403" in error_msg:  # IPè¢«å°
                logger.error("IPè¢«å°ï¼Œåˆ‡æ¢ä»£ç†...")
                # æ ‡è®°å½“å‰ä»£ç†å¤±è´¥
                # è‡ªåŠ¨åˆ‡æ¢åˆ°ä¸‹ä¸€ä¸ªä»£ç†
                await asyncio.sleep(5)

            elif "captcha" in error_msg.lower():  # éªŒè¯ç 
                logger.error("é‡åˆ°éªŒè¯ç ï¼Œéœ€è¦äººå·¥å¤„ç†")
                raise  # ä¸è‡ªåŠ¨é‡è¯•

            else:
                wait_time = backoff_factor ** attempt
                logger.warning(f"è¯·æ±‚å¤±è´¥: {error_msg}ï¼Œç­‰å¾… {wait_time}ç§’åé‡è¯•...")
                await asyncio.sleep(wait_time)

            if attempt == max_retries - 1:
                logger.error(f"é‡è¯• {max_retries} æ¬¡åä»ç„¶å¤±è´¥")
                raise

    raise Exception("é‡è¯•æ¬¡æ•°è€—å°½")
```

### 3. åº”æ€¥é¢„æ¡ˆ

```python
class EmergencyHandler:
    """åº”æ€¥å¤„ç†å™¨"""

    @staticmethod
    async def on_ip_banned(crawler):
        """IPè¢«å°æ—¶çš„å¤„ç†"""
        logger.error("æ£€æµ‹åˆ°IPè¢«å°ï¼Œæ‰§è¡Œåº”æ€¥é¢„æ¡ˆ...")

        # 1. åœæ­¢çˆ¬è™«
        await crawler.delay()

        # 2. åˆ‡æ¢ä»£ç†
        if crawler.proxy_pool:
            # æ ‡è®°å½“å‰ä»£ç†å¤±è´¥
            current_proxy = await crawler.proxy_pool.get_proxy()
            await crawler.proxy_pool.report_failure(current_proxy)

            # å¥åº·æ£€æŸ¥ï¼Œæ¿€æ´»æ–°ä»£ç†
            await crawler.proxy_pool.health_check()

        # 3. å»¶é•¿ç­‰å¾…æ—¶é—´
        crawler.request_delay = (
            crawler.request_delay[0] * 2,
            crawler.request_delay[1] * 2
        )
        logger.info(f"å»¶é•¿å»¶è¿Ÿæ—¶é—´è‡³: {crawler.request_delay}")

        # 4. é€šçŸ¥ç®¡ç†å‘˜
        # send_alert("çˆ¬è™«IPè¢«å°", "éœ€è¦æ·»åŠ æ–°ä»£ç†æˆ–è°ƒæ•´ç­–ç•¥")

    @staticmethod
    async def on_captcha_detected(crawler, url):
        """é‡åˆ°éªŒè¯ç æ—¶çš„å¤„ç†"""
        logger.warning(f"æ£€æµ‹åˆ°éªŒè¯ç : {url}")

        # 1. ä¿å­˜URLï¼Œç¨åæ‰‹åŠ¨å¤„ç†
        with open("captcha_urls.txt", "a") as f:
            f.write(f"{url}\n")

        # 2. æˆªå›¾ä¿å­˜
        await crawler.screenshot(url, f"captcha_{datetime.now().timestamp()}.png")

        # 3. é€šçŸ¥ç®¡ç†å‘˜
        # send_alert("é‡åˆ°éªŒè¯ç ", f"URL: {url}")

    @staticmethod
    async def on_rate_limit(crawler):
        """è§¦å‘é¢‘ç‡é™åˆ¶æ—¶çš„å¤„ç†"""
        logger.warning("è§¦å‘é¢‘ç‡é™åˆ¶ï¼Œè¿›å…¥å†·å´æœŸ...")

        # 1. æš‚åœçˆ¬è™«
        cooldown_time = 60 * 5  # 5åˆ†é’Ÿ
        logger.info(f"æš‚åœ {cooldown_time//60} åˆ†é’Ÿ")
        await asyncio.sleep(cooldown_time)

        # 2. å‡å°‘è¯·æ±‚é¢‘ç‡
        crawler.request_delay = (
            crawler.request_delay[0] * 1.5,
            crawler.request_delay[1] * 1.5
        )
```

---

## æ¡ˆä¾‹åˆ†æ

### æ¡ˆä¾‹1: è±†ç“£åçˆ¬

**åçˆ¬æœºåˆ¶**:
- é¢‘ç‡é™åˆ¶ä¸¥æ ¼
- éœ€è¦ç™»å½•Cookie
- IPå°ç¦

**åº”å¯¹ç­–ç•¥**:
```python
class DoubanCrawler(BrowserCrawler):
    def __init__(self, **kwargs):
        super().__init__(
            name="DoubanCrawler",
            request_delay=(5, 10),  # è¾ƒé•¿å»¶è¿Ÿ
            **kwargs
        )

    async def run(self):
        # 1. ä½¿ç”¨é«˜è´¨é‡ä½å®…ä»£ç†
        # 2. æºå¸¦ç™»å½•Cookie
        # 3. è®¾ç½®é•¿å»¶è¿Ÿ
        # 4. é™åˆ¶æ¯ä¸ªIPçš„è¯·æ±‚æ•°
        pass
```

### æ¡ˆä¾‹2: æ·˜å®åçˆ¬

**åçˆ¬æœºåˆ¶**:
- æå¼ºçš„JavaScriptæŒ‘æˆ˜
- æ»‘åŠ¨éªŒè¯ç 
- è®¾å¤‡æŒ‡çº¹
- è¡Œä¸ºåˆ†æ

**åº”å¯¹ç­–ç•¥**:
- ä½¿ç”¨çœŸå®æµè§ˆå™¨ï¼ˆPlaywrightï¼‰
- å®Œæ•´æ¨¡æ‹Ÿäººç±»è¡Œä¸º
- ä½¿ç”¨ä½å®…ä»£ç†
- é¿å¼€é«˜å³°æ—¶æ®µ

---

## æ€»ç»“

### âœ… åçˆ¬è™«é»„é‡‘æ³•åˆ™

1. **åˆæ³•åˆè§„ç¬¬ä¸€** - éµå®ˆæ³•å¾‹å’Œç½‘ç«™è§„åˆ™
2. **å°Šé‡æœåŠ¡å™¨** - åˆç†çš„è¯·æ±‚é¢‘ç‡
3. **ä¼ªè£…çœŸå®æ€§** - User-Agentã€Cookieã€è¡Œä¸º
4. **ä½¿ç”¨ä»£ç†** - IPè½®æ¢å’Œéšè—
5. **ç›‘æ§è°ƒæ•´** - å®æ—¶ç›‘æ§ï¼Œçµæ´»åº”å¯¹

### ğŸ“Š åçˆ¬è™«ç­–ç•¥ä¼˜å…ˆçº§

| ä¼˜å…ˆçº§ | ç­–ç•¥ | å®æ–½éš¾åº¦ | æ•ˆæœ |
|-------|------|---------|-----|
| 1 | åˆç†å»¶è¿Ÿ | â­ | â­â­â­â­ |
| 2 | User-Agentè½®æ¢ | â­ | â­â­â­ |
| 3 | ä»£ç†æ±  | â­â­â­ | â­â­â­â­â­ |
| 4 | Cookieç®¡ç† | â­â­ | â­â­â­â­ |
| 5 | è¡Œä¸ºæ¨¡æ‹Ÿ | â­â­â­â­ | â­â­â­â­ |
| 6 | éªŒè¯ç å¤„ç† | â­â­â­â­â­ | â­â­â­ |

---

**ç¥ä½ çˆ¬å–æˆåŠŸï¼Œè¿œç¦»å°ç¦ï¼** ğŸ›¡ï¸

æ›´å¤šæ–‡æ¡£: [çˆ¬è™«ä½¿ç”¨æŒ‡å—](crawler_guide.md) | [çˆ¬è™«ç¤ºä¾‹](crawler_examples.md)
