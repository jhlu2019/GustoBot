# GustoBot çˆ¬è™«æ¨¡å—

ä¼ä¸šçº§èœè°±æ•°æ®çˆ¬è™«,æ”¯æŒå¤šç§æ•°æ®æºã€ä»£ç†æ± ã€åçˆ¬è™«æœºåˆ¶ã€‚

## ğŸŒŸ ç‰¹æ€§

- âœ… **å¤šæ•°æ®æºæ”¯æŒ**: Wikipediaã€Schema.orgæ ‡å‡†ç½‘ç«™ã€é€šç”¨HTML
- âœ… **ä»£ç†æ± ç®¡ç†**: è‡ªåŠ¨è½®æ¢ã€å¥åº·æ£€æŸ¥ã€å¤±è´¥é‡è¯•
- âœ… **åçˆ¬è™«æœºåˆ¶**:
  - éšæœºUser-Agent
  - è¯·æ±‚å»¶è¿Ÿ
  - IPä»£ç†è½®æ¢
  - Robots.txtéµå®ˆ
  - é‡è¯•æœºåˆ¶
- âœ… **æ•°æ®éªŒè¯**: Pydanticæ¨¡å‹éªŒè¯ã€æ•°æ®æ¸…æ´—ã€å»é‡
- âœ… **Schema.orgæ”¯æŒ**: JSON-LDã€Microdataæ ¼å¼è§£æ
- âœ… **CLIå·¥å…·**: å‘½ä»¤è¡Œç•Œé¢æ–¹ä¾¿ä½¿ç”¨
- âœ… **å¼‚æ­¥çˆ¬å–**: åŸºäºhttpxçš„å¼‚æ­¥è¯·æ±‚

## ğŸ“¦ å®‰è£…ä¾èµ–

```bash
pip install httpx beautifulsoup4 fake-useragent pydantic
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. Wikipediaèœè°±çˆ¬å–

```python
import asyncio
from app.crawler import WikipediaCrawler

async def main():
    crawler = WikipediaCrawler(language="zh")
    recipes = await crawler.run(
        search_queries=["å·èœ", "ç²¤èœ"],
        limit_per_query=5
    )
    print(f"çˆ¬å–äº† {len(recipes)} ä¸ªèœè°±")

asyncio.run(main())
```

### 2. é€šç”¨ç½‘ç«™çˆ¬å–

```python
import asyncio
from app.crawler import RecipeCrawler

async def main():
    crawler = RecipeCrawler()
    recipes = await crawler.run([
        "https://example.com/recipe1",
        "https://example.com/recipe2"
    ])

asyncio.run(main())
```

### 3. ä½¿ç”¨ä»£ç†æ± 

```python
from app.crawler import ProxyPool, RecipeCrawler

# ä»æ–‡ä»¶åŠ è½½ä»£ç†
proxy_pool = ProxyPool.from_file("proxies.txt")

# æˆ–æ‰‹åŠ¨æ·»åŠ ä»£ç†
proxy_pool = ProxyPool()
proxy_pool.add_proxy(host="127.0.0.1", port=8080)
proxy_pool.add_proxy(
    host="proxy.example.com",
    port=8080,
    username="user",
    password="pass"
)

# ä½¿ç”¨ä»£ç†æ± 
crawler = RecipeCrawler(proxy_pool=proxy_pool)
```

## ğŸ”§ CLI ä½¿ç”¨

### Wikipediaçˆ¬å–

```bash
# åŸºç¡€ç”¨æ³•
python -m app.crawler.cli wikipedia --query "å·èœ" "ç²¤èœ"

# æŒ‡å®šè¯­è¨€å’Œæ•°é‡
python -m app.crawler.cli wikipedia --query "ä¸­å›½èœ" --language zh --limit 10

# ä½¿ç”¨ä»£ç†
python -m app.crawler.cli wikipedia --query "çƒ˜ç„™" --proxy proxies.txt

# ä¿å­˜åˆ°æ–‡ä»¶
python -m app.crawler.cli wikipedia --query "å®¶å¸¸èœ" --output recipes.json

# ç›´æ¥å¯¼å…¥åˆ°çŸ¥è¯†åº“
python -m app.crawler.cli wikipedia --query "ç”œå“" --import-kb
```

### URLçˆ¬å–

```bash
# çˆ¬å–æŒ‡å®šURL
python -m app.crawler.cli urls --urls "https://example.com/recipe1" "https://example.com/recipe2"

# ä½¿ç”¨ä»£ç†å¹¶ä¿å­˜
python -m app.crawler.cli urls --urls "https://example.com/recipes" --proxy proxies.txt --output output.json

# ç›´æ¥å¯¼å…¥çŸ¥è¯†åº“
python -m app.crawler.cli urls --urls "https://example.com/recipe" --import-kb
```

### ä»æ–‡ä»¶å¯¼å…¥

```bash
# å°†JSONæ–‡ä»¶å¯¼å…¥çŸ¥è¯†åº“
python -m app.crawler.cli import --file recipes.json --batch-size 20
```

## ğŸ“ ä»£ç†é…ç½®

åˆ›å»º `proxies.txt` æ–‡ä»¶,æ¯è¡Œä¸€ä¸ªä»£ç†:

```
# æ ¼å¼1: host:port
127.0.0.1:8080

# æ ¼å¼2: host:port:username:password
proxy.example.com:8080:user:pass

# æ ¼å¼3: protocol://host:port
http://127.0.0.1:8080

# æ ¼å¼4: protocol://username:password@host:port
http://user:pass@proxy.example.com:8080
```

## ğŸ¯ é«˜çº§ç”¨æ³•

### è‡ªå®šä¹‰çˆ¬è™«

```python
from app.crawler import BaseCrawler
import httpx

class MyCrawler(BaseCrawler):
    def __init__(self, **kwargs):
        super().__init__(name="MyCrawler", **kwargs)

    async def parse(self, response: httpx.Response):
        # å®ç°è§£æé€»è¾‘
        pass

    async def run(self, **kwargs):
        # å®ç°çˆ¬å–é€»è¾‘
        self.start_stats()

        response = await self.fetch("https://example.com")
        if response:
            recipes = await self.parse(response)

        self.end_stats()
        return recipes
```

### ä»£ç†æ± å¥åº·æ£€æŸ¥

```python
import asyncio
from app.crawler import ProxyPool

async def main():
    proxy_pool = ProxyPool.from_file("proxies.txt")

    # æ‰‹åŠ¨å¥åº·æ£€æŸ¥
    await proxy_pool.health_check()

    # å¯åŠ¨è‡ªåŠ¨å¥åº·æ£€æŸ¥(æ¯5åˆ†é’Ÿ)
    asyncio.create_task(proxy_pool.start_health_check_loop())

    # æŸ¥çœ‹ç»Ÿè®¡
    stats = proxy_pool.get_stats()
    print(stats)

asyncio.run(main())
```

### æ•°æ®éªŒè¯å’Œæ¸…æ´—

```python
from app.crawler.data_validator import DataValidator, RecipeModel

# éªŒè¯å•ä¸ªèœè°±
recipe_data = {
    "name": "çº¢çƒ§è‚‰",
    "ingredients": ["äº”èŠ±è‚‰500g", "å†°ç³–30g"],
    "steps": ["1. åˆ‡å—", "2. ç„¯æ°´"]
}

validated = DataValidator.validate(recipe_data)
if validated:
    print(f"éªŒè¯æˆåŠŸ: {validated.name}")

# æ‰¹é‡éªŒè¯
recipes = DataValidator.validate_batch(recipe_list)

# å»é‡
unique_recipes = DataValidator.deduplicate(recipes)

# æ—¶é—´è§„èŒƒåŒ–
minutes = DataValidator.normalize_time("1å°æ—¶30åˆ†é’Ÿ")  # è¿”å› 90
```

## ğŸ›¡ï¸ åçˆ¬è™«æœ€ä½³å®è·µ

### 1. åˆç†è®¾ç½®å»¶è¿Ÿ

```python
crawler = RecipeCrawler(
    request_delay=(2, 5),  # 2-5ç§’éšæœºå»¶è¿Ÿ
    max_retries=3
)
```

### 2. ä½¿ç”¨ä»£ç†æ± 

```python
proxy_pool = ProxyPool(
    check_interval=300,  # 5åˆ†é’Ÿå¥åº·æ£€æŸ¥
    max_fail_count=5     # æœ€å¤§å¤±è´¥æ¬¡æ•°
)
```

### 3. éµå®ˆRobots.txt

```python
crawler = RecipeCrawler(
    respect_robots_txt=True  # é»˜è®¤å¼€å¯
)
```

### 4. æ§åˆ¶å¹¶å‘

é¿å…åŒæ—¶å‘èµ·å¤§é‡è¯·æ±‚,ä½¿ç”¨å¼‚æ­¥ä½†æ§åˆ¶å¹¶å‘æ•°:

```python
import asyncio

async def crawl_with_semaphore(urls, max_concurrent=3):
    semaphore = asyncio.Semaphore(max_concurrent)

    async def fetch_one(url):
        async with semaphore:
            return await crawler.fetch(url)

    tasks = [fetch_one(url) for url in urls]
    return await asyncio.gather(*tasks)
```

## ğŸ“Š æ•°æ®æ ¼å¼

### è¾“å‡ºæ ¼å¼

```json
{
  "name": "çº¢çƒ§è‚‰",
  "description": "ç»å…¸ä¸­å¼èœè‚´,è‰²æ³½çº¢äº®,è‚¥è€Œä¸è…»",
  "category": "å®¶å¸¸èœ",
  "cuisine": "ä¸­å›½èœ",
  "difficulty": "ä¸­ç­‰",
  "ingredients": [
    "äº”èŠ±è‚‰500g",
    "å†°ç³–30g",
    "ç”ŸæŠ½2å‹º",
    "è€æŠ½1å‹º"
  ],
  "steps": [
    "1. äº”èŠ±è‚‰åˆ‡å—,å†·æ°´ä¸‹é”…ç„¯æ°´",
    "2. ç‚’ç³–è‰²,åŠ å…¥äº”èŠ±è‚‰ç¿»ç‚’ä¸Šè‰²",
    "3. åŠ å…¥è°ƒæ–™å’Œçƒ­æ°´,å°ç«ç‚–ç…®40åˆ†é’Ÿ"
  ],
  "time": {
    "prep": "PT15M",
    "cook": "PT45M",
    "total": "PT1H"
  },
  "servings": "4äººä»½",
  "nutrition": {
    "calories": "450kcal",
    "protein": "20g",
    "carbs": "15g",
    "fat": "35g"
  },
  "tips": "ç³–è‰²ä¸è¦ç‚’è¿‡å¤´,å®¹æ˜“å‘è‹¦",
  "image": "https://example.com/image.jpg",
  "author": "Chef Zhang",
  "source": "Wikipedia",
  "url": "https://zh.wikipedia.org/wiki/çº¢çƒ§è‚‰"
}
```

## âš ï¸ æ³¨æ„äº‹é¡¹

1. **æ³•å¾‹åˆè§„**: ä»…çˆ¬å–å…¬å¼€æ•°æ®,éµå®ˆç½‘ç«™çš„robots.txtå’ŒæœåŠ¡æ¡æ¬¾
2. **è¯·æ±‚é¢‘ç‡**: é¿å…è¿‡é«˜çš„è¯·æ±‚é¢‘ç‡,å»ºè®®è®¾ç½®åˆç†çš„å»¶è¿Ÿ
3. **æ•°æ®ä½¿ç”¨**: çˆ¬å–çš„æ•°æ®ä»…ç”¨äºä¸ªäººå­¦ä¹ æˆ–åˆæ³•å•†ä¸šç”¨é€”
4. **ä»£ç†ä½¿ç”¨**: ä½¿ç”¨åˆæ³•çš„ä»£ç†æœåŠ¡,ä¸è¦ä½¿ç”¨éæ³•ä»£ç†
5. **é”™è¯¯å¤„ç†**: åšå¥½å¼‚å¸¸å¤„ç†,é¿å…çˆ¬è™«å¼‚å¸¸é€€å‡º

## ğŸ” æ”¯æŒçš„ç½‘ç«™ç±»å‹

### âœ… å®Œå…¨æ”¯æŒ

- Wikipedia (å¤šè¯­è¨€)
- å®ç°Schema.org Recipeæ ‡å‡†çš„ç½‘ç«™
- åŒ…å«JSON-LDç»“æ„åŒ–æ•°æ®çš„ç½‘ç«™
- ä½¿ç”¨Microdataæ ¼å¼çš„ç½‘ç«™

### âš¡ éƒ¨åˆ†æ”¯æŒ

- é€šç”¨HTMLç»“æ„çš„èœè°±ç½‘ç«™(ä½¿ç”¨å¯å‘å¼è§£æ)
- éœ€è¦JavaScriptæ¸²æŸ“çš„ç½‘ç«™(éœ€é…åˆSeleniumä½¿ç”¨)

### âŒ ä¸æ”¯æŒ

- éœ€è¦ç™»å½•çš„ç½‘ç«™
- æœ‰ä¸¥æ ¼åçˆ¬æªæ–½çš„ç½‘ç«™
- åŠ¨æ€åŠ è½½å†…å®¹çš„SPAåº”ç”¨(é™¤éé…åˆæµè§ˆå™¨è‡ªåŠ¨åŒ–)

## ğŸ“š ç›¸å…³èµ„æº

- [Schema.org Recipe](https://schema.org/Recipe)
- [Wikipedia API](https://www.mediawiki.org/wiki/API:Main_page)
- [httpx Documentation](https://www.python-httpx.org/)
- [BeautifulSoup Documentation](https://www.crummy.com/software/BeautifulSoup/)

## ğŸ¤ è´¡çŒ®

æ¬¢è¿æäº¤Issueå’ŒPull Requestæ¥æ”¹è¿›çˆ¬è™«åŠŸèƒ½!

## ğŸ“„ è®¸å¯è¯

Apache License 2.0
