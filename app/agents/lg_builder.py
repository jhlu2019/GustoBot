from app.agents.lg_states import AgentState, Router
from app.agents.lg_prompts import (
    ROUTER_SYSTEM_PROMPT,
    GET_ADDITIONAL_SYSTEM_PROMPT,
    GENERAL_QUERY_SYSTEM_PROMPT,
    GET_IMAGE_SYSTEM_PROMPT,
    GUARDRAILS_SYSTEM_PROMPT,
    RAGSEARCH_SYSTEM_PROMPT,
    CHECK_HALLUCINATIONS,
    GENERATE_QUERIES_SYSTEM_PROMPT
)
from langchain_core.runnables import RunnableConfig
from app.config import settings
from app.core.logger import get_logger
from typing import cast, Literal, TypedDict, List, Dict, Any
from langchain_core.messages import BaseMessage
from langgraph.checkpoint.memory import MemorySaver
from langgraph.graph import END, START, StateGraph
from app.agents.lg_states import AgentState, InputState, Router, GradeHallucinations
from app.agents.kg_sub_graph.agentic_rag_agents.retrievers.cypher_examples.recipe_retriever import \
    RecipeCypherRetriever
from app.agents.kg_sub_graph.agentic_rag_agents.components.planner.node import create_planner_node
from app.agents.kg_sub_graph.agentic_rag_agents.workflows.multi_agent.multi_tool import create_multi_tool_workflow
from app.agents.kg_sub_graph.kg_neo4j_conn import get_neo4j_graph
from pydantic import BaseModel
from typing import Dict, List
from langchain_core.messages import AIMessage
from langchain_core.runnables.base import Runnable
from app.agents.kg_sub_graph.agentic_rag_agents.components.utils.utils import \
    retrieve_and_parse_schema_from_graph_for_prompts
from langchain_core.prompts import ChatPromptTemplate
import base64
import os
import aiohttp
import asyncio
import json
import time
from pathlib import Path
from PIL import Image
import io

from typing import Literal, Optional
from pydantic import BaseModel, Field
from langchain_openai import ChatOpenAI
class AdditionalGuardrailsOutput(BaseModel):
    """
    æ ¼å¼åŒ–è¾“å‡ºï¼Œç”¨äºåˆ¤æ–­ç”¨æˆ·çš„é—®é¢˜æ˜¯å¦ä¸å›¾è°±å†…å®¹ç›¸å…³
    """
    decision: Literal["end", "proceed"] = Field(
        description="Decision on whether the question is related to the graph contents."
    )


# æ„å»ºæ—¥å¿—è®°å½•å™¨
logger = get_logger(service="lg_builder")


async def analyze_and_route_query(
        state: AgentState, *, config: RunnableConfig
) -> dict[str, Router]:
    """Analyze the user's query and determine the appropriate routing.

    This function uses a language model to classify the user's query and decide how to route it
    within the conversation flow.

    Args:
        state (AgentState): The current state of the agent, including conversation history.
        config (RunnableConfig): Configuration with the model used for query analysis.

    Returns:
        dict[str, Router]: A dictionary containing the 'router' key with the classification result (classification type and logic).
    """

    model = ChatOpenAI(openai_api_key=settings.OPENAI_API_KEY,model_name=settings.OPENAI_MODEL,openai_api_base=settings.OPENAI_API_BASE, temperature=0.7,
                       tags=["router"])

    # æ‹¼æ¥æç¤ºæ¨¡ç‰ˆ + ç”¨æˆ·çš„å®æ—¶é—®é¢˜ï¼ˆåŒ…å«å†å²ä¸Šä¸‹æ–‡å¯¹è¯ï¼‰ 
    messages = [
                   {"role": "system", "content": ROUTER_SYSTEM_PROMPT}
               ] + state.messages
    logger.info("-----Analyze user query type-----")
    logger.info(f"History messages: {state.messages}")

    # ä½¿ç”¨ç»“æ„åŒ–è¾“å‡ºï¼Œè¾“å‡ºé—®é¢˜ç±»å‹
    response = cast(
        Router, await model.with_structured_output(Router).ainvoke(messages)
    )
    logger.info(f"Analyze user query type completed, result: {response}")
    return {"router": response}


def route_query(
        state: AgentState,
) -> Literal[
    "respond_to_general_query", "get_additional_info", "create_research_plan", "create_image_query", "create_file_query"]:
    """æ ¹æ®æŸ¥è¯¢åˆ†ç±»ç¡®å®šä¸‹ä¸€æ­¥æ“ä½œã€‚

    Args:
        state (AgentState): å½“å‰ä»£ç†çŠ¶æ€ï¼ŒåŒ…æ‹¬è·¯ç”±å™¨çš„åˆ†ç±»ã€‚

    Returns:
        Literal["respond_to_general_query", "get_additional_info", "create_research_plan", "create_image_query", "create_file_query"]: ä¸‹ä¸€æ­¥æ“ä½œã€‚
    """
    _type = state.router["type"]

    # æ£€æŸ¥é…ç½®ä¸­æ˜¯å¦æœ‰å›¾ç‰‡è·¯å¾„ï¼Œå¦‚æœæœ‰ï¼Œä¼˜å…ˆå¤„ç†ä¸ºå›¾ç‰‡æŸ¥è¯¢
    if hasattr(state, "config") and state.config and state.config.get("configurable", {}).get("image_path"):
        logger.info("æ£€æµ‹åˆ°å›¾ç‰‡è·¯å¾„ï¼Œè½¬ä¸ºå›¾ç‰‡æŸ¥è¯¢å¤„ç†")
        return "create_image_query"

    if _type == "general-query":
        return "respond_to_general_query"
    elif _type == "additional-query":
        return "get_additional_info"
    elif _type == "graphrag-query":  # å®é™…ä½¿ç”¨ LightRAG (è½»é‡çº§æ›¿ä»£æ–¹æ¡ˆ)
        return "create_research_plan"
    elif _type == "image-query":
        return "create_image_query"
    elif _type == "file-query":
        return "create_file_query"
    else:
        raise ValueError(f"Unknown router type {_type}")


async def respond_to_general_query(
        state: AgentState, *, config: RunnableConfig
) -> Dict[str, List[BaseMessage]]:
    """ç”Ÿæˆå¯¹ä¸€èˆ¬æŸ¥è¯¢çš„å“åº”ï¼Œå®Œå…¨åŸºäºå¤§æ¨¡å‹ï¼Œä¸ä¼šè§¦å‘ä»»ä½•å¤–éƒ¨æœåŠ¡çš„è°ƒç”¨ï¼ŒåŒ…æ‹¬è‡ªå®šä¹‰å·¥å…·ã€çŸ¥è¯†åº“æŸ¥è¯¢ç­‰ã€‚
    å½“è·¯ç”±å™¨å°†æŸ¥è¯¢åˆ†ç±»ä¸ºä¸€èˆ¬é—®é¢˜æ—¶ï¼Œå°†è°ƒç”¨æ­¤èŠ‚ç‚¹ã€‚
    Args:
        state (AgentState): å½“å‰ä»£ç†çŠ¶æ€ï¼ŒåŒ…æ‹¬å¯¹è¯å†å²å’Œè·¯ç”±é€»è¾‘ã€‚
        config (RunnableConfig): ç”¨äºé…ç½®å“åº”ç”Ÿæˆçš„æ¨¡å‹ã€‚
    Returns:
        Dict[str, List[BaseMessage]]: åŒ…å«'messages'é”®çš„å­—å…¸ï¼Œå…¶ä¸­åŒ…å«ç”Ÿæˆçš„å“åº”ã€‚
    """
    logger.info("-----generate general-query response-----")

    # ä½¿ç”¨å¤§æ¨¡å‹ç”Ÿæˆå›å¤
    model = ChatOpenAI(openai_api_key=settings.OPENAI_API_KEY, model_name=settings.OPENAI_MODEL,
                       openai_api_base=settings.OPENAI_API_BASE, temperature=0.7,
                       tags=["general_query"])

    system_prompt = GENERAL_QUERY_SYSTEM_PROMPT.format(
        logic=state.router["logic"]
    )

    messages = [{"role": "system", "content": system_prompt}] + state.messages
    response = await model.ainvoke(messages)
    return {"messages": [response]}

#å¤§æ¨¡å‹ç”Ÿæˆè¾“å‡ºå¤šäº†ä¸€äº›é¢å¤–æ¶ˆæ¯
async def get_additional_info(
        state: AgentState, *, config: RunnableConfig
) -> Dict[str, List[BaseMessage]]:
    """ç”Ÿæˆä¸€ä¸ªå“åº”ï¼Œè¦æ±‚ç”¨æˆ·æä¾›æ›´å¤šä¿¡æ¯ã€‚

    å½“è·¯ç”±ç¡®å®šéœ€è¦ä»ç”¨æˆ·é‚£é‡Œè·å–æ›´å¤šä¿¡æ¯æ—¶ï¼Œå°†è°ƒç”¨æ­¤å‡½æ•°ã€‚

    Args:
        state (AgentState): å½“å‰ä»£ç†çŠ¶æ€ï¼ŒåŒ…æ‹¬å¯¹è¯å†å²å’Œè·¯ç”±é€»è¾‘ã€‚
        config (RunnableConfig): ç”¨äºé…ç½®å“åº”ç”Ÿæˆçš„æ¨¡å‹ã€‚

    Returns:
        Dict[str, List[BaseMessage]]: åŒ…å«'messages'é”®çš„å­—å…¸ï¼Œå…¶ä¸­åŒ…å«ç”Ÿæˆçš„å“åº”ã€‚
    """
    logger.info("------continue to get additional info------")

    # ä½¿ç”¨å¤§æ¨¡å‹ç”Ÿæˆå›å¤
    model = ChatOpenAI(openai_api_key=settings.OPENAI_API_KEY, model_name=settings.OPENAI_MODEL,
                       openai_api_base=settings.OPENAI_API_BASE, temperature=0.7,
                       tags=["additional_info"])
    # å¦‚æœç”¨æˆ·çš„é—®é¢˜æ˜¯èœè°±ç›¸å…³ï¼Œä½†ä¸è‡ªå·±çš„ä¸šåŠ¡æ— å…³ï¼Œåˆ™éœ€è¦è¿”å›"æ— å…³é—®é¢˜"

    # é¦–å…ˆè¿æ¥ Neo4j å›¾æ•°æ®åº“
    try:
        neo4j_graph = get_neo4j_graph()
        logger.info("success to get Neo4j graph database connection")
    except Exception as e:
        logger.error(f"failed to get Neo4j graph database connection: {e}")
        neo4j_graph = None

    # å®šä¹‰èœè°±åŠ©æ‰‹æœåŠ¡èŒƒå›´ï¼ˆç”¨æˆ·å‹å¥½çš„ä¸šåŠ¡æè¿°ï¼‰
    scope_description = """
    èœè°±æ™ºèƒ½åŠ©æ‰‹æœåŠ¡èŒƒå›´ï¼šä¸ºæ‚¨æä¾›å…¨æ–¹ä½çš„çƒ¹é¥ªæŒ‡å¯¼å’Œç¾é£ŸçŸ¥è¯†ï¼ŒåŒ…æ‹¬ä½†ä¸é™äºï¼š

    ğŸ³ èœè°±æŸ¥è¯¢ä¸åˆ¶ä½œæŒ‡å¯¼
    - å„ç±»ä¸­åæ–™ç†çš„è¯¦ç»†åšæ³•å’Œçƒ¹é¥ªæŠ€å·§
    - é£Ÿæç”¨é‡ã€çƒ¹é¥ªæ—¶é•¿ã€ç«å€™æŒæ¡
    - åˆ†æ­¥éª¤çš„çƒ¹é¥ªæŒ‡å¯¼å’Œå°è´´å£«

    ğŸ¥¬ é£ŸæçŸ¥è¯†ä¸è¥å…»ä»·å€¼
    - é£Ÿæçš„è¥å…»æˆåˆ†å’Œå¥åº·åŠŸæ•ˆ
    - é£Ÿæçš„é€‰è´­ã€å‚¨å­˜å’Œå¤„ç†æ–¹æ³•
    - é£Ÿæä¹‹é—´çš„æ­é…å’Œæ›¿ä»£å»ºè®®

    ğŸŒ¶ï¸ å£å‘³ä¸çƒ¹é¥ªæŠ€æ³•
    - å„ç§å£å‘³ç‰¹ç‚¹ï¼ˆéº»è¾£ã€é…±é¦™ã€æ¸…æ·¡ç­‰ï¼‰
    - ä¸åŒçƒ¹é¥ªæ–¹æ³•ï¼ˆç‚’ã€è’¸ã€ç…®ã€ç‚–ã€çƒ¤ç­‰ï¼‰
    - èœå“åˆ†ç±»ï¼ˆçƒ­èœã€å‡‰èœã€æ±¤å“ã€ä¸»é£Ÿç­‰ï¼‰

    ğŸ’Š é£Ÿç–—å…»ç”Ÿå»ºè®®
    - é£Ÿæçš„ä¸­åŒ»é£Ÿç–—åŠŸæ•ˆ
    - å­£èŠ‚æ€§é¥®é£Ÿè°ƒç†å»ºè®®
    - ç‰¹å®šäººç¾¤çš„é¥®é£Ÿæ³¨æ„äº‹é¡¹

    æš‚ä¸æ”¯æŒï¼šæ”¿æ²»ã€å¨±ä¹å…«å¦ã€æ–°é—»æ—¶äº‹ã€å¤©æ°”é¢„æŠ¥ã€ç½‘è´­æ¨èã€åŒ»ç–—è¯Šæ–­ç­‰éçƒ¹é¥ªç¾é£Ÿç›¸å…³å†…å®¹ã€‚
    å¦‚é‡æ­¤ç±»é—®é¢˜ï¼Œæˆ‘ä¼šç¤¼è²Œåœ°å¼•å¯¼æ‚¨å›åˆ°çƒ¹é¥ªç¾é£Ÿè¯é¢˜ï½
    """

    scope_context = (
        f"å‚è€ƒæ­¤èŒƒå›´æè¿°æ¥å†³ç­–:\n{scope_description}"
        if scope_description is not None
        else ""
    )

    # åŠ¨æ€ä» Neo4j å›¾è¡¨ä¸­è·å–å›¾è¡¨ç»“æ„
    graph_context = (
        f"\nå‚è€ƒå›¾è¡¨ç»“æ„æ¥å›ç­”:\n{retrieve_and_parse_schema_from_graph_for_prompts(neo4j_graph)}"
        if neo4j_graph is not None
        else ""
    )

    message = scope_context + graph_context + "\nQuestion: {question}"

    # æ‹¼æ¥æç¤ºæ¨¡ç‰ˆ
    full_system_prompt = ChatPromptTemplate.from_messages(
        [
            (
                "system",
                GUARDRAILS_SYSTEM_PROMPT,
            ),
            (
                "human",
                (message),
            ),
        ]
    )

    # æ„å»ºæ ¼å¼åŒ–è¾“å‡ºçš„ Chainï¼Œ å¦‚æœåŒ¹é…ï¼Œè¿”å› continueï¼Œå¦åˆ™è¿”å› end
    guardrails_chain = full_system_prompt | model.with_structured_output(AdditionalGuardrailsOutput)
    guardrails_output: AdditionalGuardrailsOutput = await guardrails_chain.ainvoke(
        {"question": state.messages[-1].content if state.messages else ""}
    )

    # æ ¹æ®æ ¼å¼åŒ–è¾“å‡ºçš„ç»“æœï¼Œè¿”å›ä¸åŒçš„å“åº”
    if guardrails_output.decision == "end":
        logger.info("-----Fail to pass guardrails check-----")
        return {"messages": [AIMessage(content="å¨å‹æ‚¨å¥½ï½æŠ±æ­‰å“¦ï¼Œè¿™ä¸ªé—®é¢˜ä¸å¤ªå±äºæˆ‘ä»¬çš„èœè°±èŒƒå›´å‘¢ï¼Œæˆ‘ä¸»è¦å¸®æ‚¨è§£ç­”èœè°±å’Œçƒ¹é¥ªæ–¹é¢çš„é—®é¢˜ï½ğŸ˜Š")]}
    else:
        logger.info("-----Pass guardrails check-----")
        system_prompt = GET_ADDITIONAL_SYSTEM_PROMPT.format(
            logic=state.router["logic"]
        )
        messages = [{"role": "system", "content": system_prompt}] + state.messages
        response = await model.ainvoke(messages)
        return {"messages": [response]}


async def create_image_query(
        state: AgentState, *, config: RunnableConfig
) -> Dict[str, List[BaseMessage]]:
    """å¤„ç†å›¾ç‰‡æŸ¥è¯¢å¹¶ç”Ÿæˆæè¿°å›å¤

    Args:
        state (AgentState): å½“å‰ä»£ç†çŠ¶æ€ï¼ŒåŒ…æ‹¬å¯¹è¯å†å²
        config (RunnableConfig): é…ç½®å‚æ•°ï¼ŒåŒ…å«çº¿ç¨‹IDç­‰é…ç½®ä¿¡æ¯

    Returns:
        Dict[str, List[BaseMessage]]: åŒ…å«'messages'é”®çš„å­—å…¸ï¼Œå…¶ä¸­åŒ…å«ç”Ÿæˆçš„å“åº”
    """
    logger.info("-----Found User Upload Image-----")
    image_path = config.get("configurable", {}).get("image_path", None)

    if not image_path:
        logger.warning(f"User Upload Image Path is None")
        return {"messages": [AIMessage(content="æŠ±æ­‰ï¼Œæˆ‘æ— æ³•æŸ¥çœ‹è¿™å¼ å›¾ç‰‡ï¼Œè¯·é‡æ–°ä¸Šä¼ ã€‚")]}

    if not Path(image_path).exists():
        logger.warning(f"User Upload Image Not Found: {image_path}")
        return {"messages": [AIMessage(content="æŠ±æ­‰ï¼Œæˆ‘æ— æ³•æŸ¥çœ‹è¿™å¼ å›¾ç‰‡ï¼Œè¯·é‡æ–°ä¸Šä¼ ã€‚")]}

    # è·å–è§†è§‰æ¨¡å‹é…ç½®
    api_key = settings.VISION_API_KEY
    base_url = settings.VISION_BASE_URL
    vision_model = settings.VISION_MODEL

    if not api_key or not base_url or not vision_model:
        logger.error("Vision Model Configuration Not Complete")
        return {"messages": [AIMessage(content="æŠ±æ­‰ï¼Œæˆ‘æ— æ³•æŸ¥çœ‹è¿™å¼ å›¾ç‰‡ï¼Œè¯·é‡æ–°ä¸Šä¼ ã€‚")]}

    logger.info(f"Using Vision Model: {vision_model} to process image: {image_path}")

    try:
        # è¯»å–å¹¶å‹ç¼©å›¾ç‰‡
        with Image.open(image_path) as img:
            # è®¾ç½®æœ€å¤§å°ºå¯¸
            max_size = 1024
            # è®¡ç®—ç¼©æ”¾æ¯”ä¾‹
            width, height = img.size
            ratio = min(max_size / width, max_size / height)

            # å¦‚æœå›¾ç‰‡å°ºå¯¸å·²ç»å°äºæœ€å¤§å°ºå¯¸ï¼Œä¸éœ€è¦ç¼©æ”¾
            if width <= max_size and height <= max_size:
                resized_img = img
            else:
                new_width = int(width * ratio)
                new_height = int(height * ratio)
                resized_img = img.resize((new_width, new_height), Image.LANCZOS)

            # è½¬æ¢ä¸ºJPEGæ ¼å¼ï¼Œå¹¶è°ƒæ•´è´¨é‡
            img_byte_arr = io.BytesIO()
            if resized_img.mode != 'RGB':
                resized_img = resized_img.convert('RGB')
            resized_img.save(img_byte_arr, format='JPEG', quality=85)
            img_byte_arr.seek(0)

            # è½¬æ¢ä¸ºbase64
            image_data = base64.b64encode(img_byte_arr.read()).decode('utf-8')

            logger.info(
                f"Image Compressed, Original Size: {width}x{height}, New Size: {resized_img.width}x{resized_img.height}")

        # æ„å»ºAPIè¯·æ±‚
        headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {api_key}"
        }

        payload = {
            "model": vision_model,
            "messages": [
                {
                    "role": "system",
                    "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„èœè°±å›¾åƒåˆ†æåŠ©æ‰‹ã€‚è¯·è¯¦ç»†åˆ†æå›¾ç‰‡ä¸­çš„å†…å®¹ï¼Œç‰¹åˆ«å…³æ³¨èœå“åç§°ã€é£Ÿæã€çƒ¹é¥ªæ–¹æ³•ã€æ‘†ç›˜ç­‰ç»†èŠ‚ã€‚"
                },
                {
                    "role": "user",
                    "content": [
                        {
                            "type": "image_url",
                            "image_url": {
                                "url": f"data:image/jpeg;base64,{image_data}"
                            }
                        }
                    ]
                }
            ],
            "max_tokens": 4000,
            "temperature": 0.7
        }

        # å‘é€APIè¯·æ±‚
        async with aiohttp.ClientSession() as session:
            async with session.post(
                    f"{base_url}/chat/completions",
                    headers=headers,
                    json=payload,
                    timeout=60  # å¢åŠ è¶…æ—¶æ—¶é—´
            ) as response:
                if response.status == 200:
                    result = await response.json()
                    image_description = result["choices"][0]["message"]["content"]
                    logger.info(f"Successfully processed image and generated description")
                    # ä½¿ç”¨å›¾ç‰‡æè¿°å’Œç”¨æˆ·é—®é¢˜ç”Ÿæˆæœ€ç»ˆå›å¤
                    # ä»lg_promptså¯¼å…¥èœè°±åŠ©æ‰‹æ¨¡æ¿

                    # æ„å»ºå›å¤è¯·æ±‚
                    model = ChatOpenAI(openai_api_key=settings.OPENAI_API_KEY, model_name=settings.OPENAI_MODEL,
                                       openai_api_base=settings.OPENAI_API_BASE, temperature=0.7,
                                       tags=["image_query"])
                    # ä½¿ç”¨ä¸“é—¨çš„å›¾ç‰‡æŸ¥è¯¢æç¤ºæ¨¡æ¿
                    system_prompt = GET_IMAGE_SYSTEM_PROMPT.format(
                        image_description=image_description
                    )
                    messages = [{"role": "system", "content": system_prompt}] + state.messages
                    response = await model.ainvoke(messages)
                    return {"messages": [response]}

                else:
                    error_text = await response.text()
                    logger.error(f"Vision API Request Failed: {response.status} - {error_text}")
                    return {"messages": [AIMessage(content=f"æŠ±æ­‰ï¼Œæˆ‘æ— æ³•æŸ¥çœ‹è¿™å¼ å›¾ç‰‡ï¼Œè¯·é‡æ–°ä¸Šä¼ ã€‚")]}




    except Exception as e:
        logger.error(f"Error processing image: {str(e)}")
        return {"messages": [AIMessage(content=f"æŠ±æ­‰ï¼Œæˆ‘æ— æ³•æŸ¥çœ‹è¿™å¼ å›¾ç‰‡ï¼Œè¯·é‡æ–°ä¸Šä¼ ã€‚")]}


async def create_file_query(
        state: AgentState, *, config: RunnableConfig
) -> Dict[str, List[BaseMessage]]:
    """Create a file query."""

    # TODO

# å›¾å·¥å…· æŸ¥è¯¢èŠ‚ç‚¹
async def create_research_plan(
        state: AgentState, *, config: RunnableConfig
) -> Dict[str, List[str] | str]:
    """é€šè¿‡æŸ¥è¯¢æœ¬åœ°çŸ¥è¯†åº“å›ç­”å®¢æˆ·é—®é¢˜ï¼Œæ‰§è¡Œä»»åŠ¡åˆ†è§£ï¼Œåˆ›å»ºåˆ†å¸ƒæŸ¥è¯¢è®¡åˆ’ã€‚

    Args:
        state (AgentState): å½“å‰ä»£ç†çŠ¶æ€ï¼ŒåŒ…æ‹¬å¯¹è¯å†å²ã€‚
        config (RunnableConfig): ç”¨äºé…ç½®è®¡åˆ’ç”Ÿæˆçš„æ¨¡å‹ã€‚

    Returns:
        Dict[str, List[str] | str]: åŒ…å«'steps'é”®çš„å­—å…¸ï¼Œå…¶ä¸­åŒ…å«ç ”ç©¶æ­¥éª¤åˆ—è¡¨ã€‚
    """
    logger.info("------execute local knowledge base query------")

    # ä½¿ç”¨å¤§æ¨¡å‹ç”ŸæˆæŸ¥è¯¢/å¤šè·³ã€å¹¶è¡ŒæŸ¥è¯¢è®¡åˆ’
    model = ChatOpenAI(openai_api_key=settings.OPENAI_API_KEY, model_name=settings.OPENAI_MODEL,
                       openai_api_base=settings.OPENAI_API_BASE, temperature=0.7,
                       tags=["research_plan"])

    # åˆå§‹åŒ–å¿…è¦å‚æ•°
    # 1. Neo4jå›¾æ•°æ®åº“è¿æ¥ - ä½¿ç”¨é…ç½®ä¸­çš„è¿æ¥ä¿¡æ¯
    neo4j_graph=None
    try:
        neo4j_graph = get_neo4j_graph()
        logger.info("success to get Neo4j graph database connection")
    except Exception as e:
        logger.error(f"failed to get Neo4j graph database connection: {e}")

    #  åˆ›å»ºèœè°±åœºæ™¯çš„æ£€ç´¢å™¨å®ä¾‹ï¼Œæ ¹æ® Graph Schemaåˆ›å»º Cypher ï¼Œ ä¼˜å…ˆç”Ÿæˆå¯¹åº”é—®é¢˜çš„cypheræ¨¡ç‰ˆ ç”¨æ¥å¼•å¯¼å¤§æ¨¡å‹ç”Ÿæˆæ­£ç¡®çš„CypheræŸ¥è¯¢è¯­å¥
    cypher_retriever = RecipeCypherRetriever()

    #  å®šä¹‰å·¥å…·æ¨¡å¼åˆ—è¡¨
    from app.agents.kg_sub_graph.kg_tools_list import cypher_query, predefined_cypher, microsoft_graphrag_query
    tool_schemas: List[type[BaseModel]] = [cypher_query, predefined_cypher, microsoft_graphrag_query]

    #  é¢„å®šä¹‰çš„CypheræŸ¥è¯¢ ä¸ºèœè°±åœºæ™¯å®šä¹‰æœ‰ç”¨çš„æŸ¥è¯¢
    from app.agents.kg_sub_graph.agentic_rag_agents.components.predefined_cypher.cypher_dict import \
        predefined_cypher_dict

    # å®šä¹‰èœè°±åŠ©æ‰‹æœåŠ¡èŒƒå›´
    scope_description = """
    èœè°±æ™ºèƒ½åŠ©æ‰‹æœåŠ¡èŒƒå›´ï¼šä¸ºæ‚¨æä¾›å…¨æ–¹ä½çš„çƒ¹é¥ªæŒ‡å¯¼å’Œç¾é£ŸçŸ¥è¯†ï¼ŒåŒ…æ‹¬ä½†ä¸é™äºï¼š

    ğŸ³ èœè°±æŸ¥è¯¢ä¸åˆ¶ä½œæŒ‡å¯¼
    - å„ç±»ä¸­åæ–™ç†çš„è¯¦ç»†åšæ³•å’Œçƒ¹é¥ªæŠ€å·§
    - é£Ÿæç”¨é‡ã€çƒ¹é¥ªæ—¶é•¿ã€ç«å€™æŒæ¡
    - åˆ†æ­¥éª¤çš„çƒ¹é¥ªæŒ‡å¯¼å’Œå°è´´å£«

    ğŸ¥¬ é£ŸæçŸ¥è¯†ä¸è¥å…»ä»·å€¼
    - é£Ÿæçš„è¥å…»æˆåˆ†å’Œå¥åº·åŠŸæ•ˆ
    - é£Ÿæçš„é€‰è´­ã€å‚¨å­˜å’Œå¤„ç†æ–¹æ³•
    - é£Ÿæä¹‹é—´çš„æ­é…å’Œæ›¿ä»£å»ºè®®

    ğŸŒ¶ï¸ å£å‘³ä¸çƒ¹é¥ªæŠ€æ³•
    - å„ç§å£å‘³ç‰¹ç‚¹ï¼ˆéº»è¾£ã€é…±é¦™ã€æ¸…æ·¡ç­‰ï¼‰
    - ä¸åŒçƒ¹é¥ªæ–¹æ³•ï¼ˆç‚’ã€è’¸ã€ç…®ã€ç‚–ã€çƒ¤ç­‰ï¼‰
    - èœå“åˆ†ç±»ï¼ˆçƒ­èœã€å‡‰èœã€æ±¤å“ã€ä¸»é£Ÿç­‰ï¼‰

    ğŸ’Š é£Ÿç–—å…»ç”Ÿå»ºè®®
    - é£Ÿæçš„ä¸­åŒ»é£Ÿç–—åŠŸæ•ˆ
    - å­£èŠ‚æ€§é¥®é£Ÿè°ƒç†å»ºè®®
    - ç‰¹å®šäººç¾¤çš„é¥®é£Ÿæ³¨æ„äº‹é¡¹

    æš‚ä¸æ”¯æŒï¼šæ”¿æ²»ã€å¨±ä¹å…«å¦ã€æ–°é—»æ—¶äº‹ã€å¤©æ°”é¢„æŠ¥ã€ç½‘è´­æ¨èã€åŒ»ç–—è¯Šæ–­ç­‰éçƒ¹é¥ªç¾é£Ÿç›¸å…³å†…å®¹ã€‚
    """

    # åˆ›å»ºå¤šå·¥å…·å·¥ä½œæµ
    multi_tool_workflow = create_multi_tool_workflow(
        llm=model,
        graph=neo4j_graph,
        tool_schemas=tool_schemas,
        predefined_cypher_dict=predefined_cypher_dict,
        cypher_example_retriever=cypher_retriever,
        scope_description=scope_description,
        llm_cypher_validation=True,
    )

    # return multi_tool_workflow
    # å‡†å¤‡è¾“å…¥çŠ¶æ€
    last_message = state.messages[-1].content if state.messages else ""
    input_state = {
        "question": last_message,
        "data": [],
        "history": []
    }

    # æ‰§è¡Œå·¥ä½œæµ
    response = await multi_tool_workflow.ainvoke(input_state)
    return {"messages": [AIMessage(content=response["answer"])]}


async def check_hallucinations(
        state: AgentState, *, config: RunnableConfig
) -> dict[str, Any]:
    """Analyze the user's query and checks if the response is supported by the set of facts based on the document retrieved,
    providing a binary score result.

    This function uses a language model to analyze the user's query and gives a binary score result.

    Args:
        state (AgentState): The current state of the agent, including conversation history.
        config (RunnableConfig): Configuration with the model used for query analysis.

    Returns:
        dict[str, Router]: A dictionary containing the 'router' key with the classification result (classification type and logic).
    """
    model = ChatOpenAI(openai_api_key=settings.OPENAI_API_KEY, model_name=settings.OPENAI_MODEL,
                       openai_api_base=settings.OPENAI_API_BASE, temperature=0.7,
                       tags=["hallucinations"])

    system_prompt = CHECK_HALLUCINATIONS.format(
        documents=state.documents,
        generation=state.messages[-1]
    )

    messages = [
                   {"role": "system", "content": system_prompt}
               ] + state.messages

    logger.info("---CHECK HALLUCINATIONS---")

    response = cast(GradeHallucinations, await model.with_structured_output(GradeHallucinations).ainvoke(messages))

    return {"hallucination": response}


checkpointer = MemorySaver()

# å®šä¹‰çŠ¶æ€å›¾
builder = StateGraph(AgentState, input=InputState)
# æ·»åŠ èŠ‚ç‚¹
builder.add_node(analyze_and_route_query) # æ„å›¾è¯†åˆ«
builder.add_node(respond_to_general_query)#é»˜è®¤å›å¤
builder.add_node(get_additional_info) # å›¾ç»“æ„ä¿¡æ¯
builder.add_node("create_research_plan", create_research_plan)  # è¿™é‡Œæ˜¯å­å›¾graphrag-query
builder.add_node(create_image_query)
builder.add_node(create_file_query)

# æ·»åŠ è¾¹
builder.add_edge(START, "analyze_and_route_query")
builder.add_conditional_edges("analyze_and_route_query", route_query)

graph = builder.compile(checkpointer=checkpointer)

# from IPython.display import Image, display
# display(Image(graph.get_graph().draw_mermaid_png()))